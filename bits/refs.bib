
@book{AGOLDBERGER1964,
  title = {Econometric {{Theory}}},
  author = {GOLDBERGER, ARTHUR S. AUTOR},
  year = {1964},
  month = jan,
  publisher = {{John Wiley \& Sons, Incorporated}},
  abstract = {Basic concepts of matrix algebra; Basic conceots of statistical inference; Classical linear regression; Extension of linear regresion; Linear regression with stochastic regressors; Systems of simultaneous linear relationship.},
  googlebooks = {KZq5AAAAIAAJ},
  isbn = {978-0-471-31101-0},
  language = {en}
}

@article{CBergmeirBenitez2012,
  title = {On the Use of Cross-Validation for Time Series Predictor Evaluation},
  author = {Bergmeir, Christoph and Ben{\'i}tez, Jos{\'e} M.},
  year = {2012},
  month = may,
  volume = {191},
  pages = {192--213},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2011.12.028},
  abstract = {In time series predictor evaluation, we observe that with respect to the model selection procedure there is a gap between evaluation of traditional forecasting procedures, on the one hand, and evaluation of machine learning techniques on the other hand. In traditional forecasting, it is common practice to reserve a part from the end of each time series for testing, and to use the rest of the series for training. Thus it is not made full use of the data, but theoretical problems with respect to temporal evolutionary effects and dependencies within the data as well as practical problems regarding missing values are eliminated. On the other hand, when evaluating machine learning and other regression methods used for time series forecasting, often cross-validation is used for evaluation, paying little attention to the fact that those theoretical problems invalidate the fundamental assumptions of cross-validation. To close this gap and examine the consequences of different model selection procedures in practice, we have developed a rigorous and extensive empirical study. Six different model selection procedures, based on (i) cross-validation and (ii) evaluation using the series' last part, are used to assess the performance of four machine learning and other regression techniques on synthetic and real-world time series. No practical consequences of the theoretical flaws were found during our study, but the use of cross-validation techniques led to a more robust model selection. To make use of the ``best of both worlds'', we suggest that the use of a blocked form of cross-validation for time series evaluation became the standard procedure, thus using all available information and circumventing the theoretical problems.},
  file = {/home/peterprescott/Zotero/storage/RTPCL8EB/Bergmeir and Ben√≠tez - 2012 - On the use of cross-validation for time series pre.pdf;/home/peterprescott/Zotero/storage/QM5N3Q52/S0020025511006773.html},
  journal = {Information Sciences},
  keywords = {Cross-validation,Error measures,Machine learning,Predictor evaluation,Regression,Time series},
  language = {en},
  series = {Data {{Mining}} for {{Software Trustworthiness}}}
}

@article{CBergmeirEtAl2018,
  title = {A Note on the Validity of Cross-Validation for Evaluating Autoregressive Time Series Prediction},
  author = {Bergmeir, Christoph and Hyndman, Rob J. and Koo, Bonsoo},
  year = {2018},
  month = apr,
  volume = {120},
  pages = {70--83},
  issn = {01679473},
  doi = {10.1016/j.csda.2017.11.003},
  abstract = {One of the most widely used standard procedures for model evaluation in classification and regression is K-fold cross-validation (CV). However, when it comes to time series forecasting, because of the inherent serial correlation and potential non-stationarity of the data, its application is not straightforward and often omitted by practitioners in favour of an out-of-sample (OOS) evaluation. In this paper, we show that in the case of a purely autoregressive model, the use of standard K-fold CV is possible as long as the models considered have uncorrelated errors. Such a setup occurs, for example, when the models nest a more appropriate model. This is very common when Machine Learning methods are used for prediction, where CV in particular is suitable to control for overfitting the data. We present theoretical insights supporting our arguments. Furthermore, we present a simulation study and a real-world example where we show empirically that K-fold CV performs favourably compared to both OOS evaluation and other time-series-specific techniques such as non-dependent cross-validation.},
  file = {/home/peterprescott/Zotero/storage/69H2A2RK/Bergmeir et al. - 2018 - A note on the validity of cross-validation for eva.pdf},
  journal = {Computational Statistics \& Data Analysis},
  language = {en}
}

@article{CBoettiger2015,
  title = {An Introduction to {{Docker}} for Reproducible Research},
  author = {Boettiger, Carl},
  year = {2015},
  volume = {49},
  pages = {71--79},
  publisher = {{ACM New York, NY, USA}},
  file = {/home/peterprescott/Zotero/storage/CQ6QN9QE/Boettiger - 2015 - An introduction to Docker for reproducible researc.pdf;/home/peterprescott/Zotero/storage/323LQUY6/2723872.html},
  journal = {ACM SIGOPS Operating Systems Review},
  number = {1}
}

@misc{CCochrane2018,
  title = {Time {{Series Nested Cross}}-{{Validation}}},
  author = {Cochrane, Courtney},
  year = {2018},
  month = may,
  abstract = {This blog post discusses the pitfalls of using traditional cross-validation with time series data. Specifically, we address 1) splitting a\ldots},
  file = {/home/peterprescott/Zotero/storage/ENHQKI78/time-series-nested-cross-validation-76adba623eb9.html},
  howpublished = {https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9},
  journal = {Medium},
  language = {en}
}

@article{CShalizi,
  title = {Advanced {{Data Analysis}} from an {{Elementary Point}} of {{View}}},
  author = {Shalizi, Cosma Rohilla},
  pages = {828},
  file = {/home/peterprescott/Zotero/storage/BJ5YFTHC/Shalizi - Advanced Data Analysis from an Elementary Point of.pdf},
  language = {en}
}

@misc{ELorenz1972,
  title = {Predictability: {{Does}} the {{Flap}} of a {{Butterfly}}'s {{Wings}} in {{Brazil}} Set off a {{Tornado}} in {{Texas}}},
  author = {Lorenz, E.N.},
  year = {1972},
  file = {/home/peterprescott/Zotero/storage/7IGFTMUL/Butterfly_1972.pdf}
}

@misc{FPerezGranger2015,
  title = {Project {{Jupyter}}: {{Computational}} Narratives as the Engine of Collaborative Data Science},
  shorttitle = {Project {{Jupyter}}},
  author = {Perez, Fernando and Granger, Brian E.},
  year = {2015},
  file = {/home/peterprescott/Zotero/storage/MLRG3DRB/Perez and Granger - 2015 - Project Jupyter Computational narratives as the e.pdf}
}

@article{FSchorfheideWolpin2012,
  title = {On the {{Use}} of {{Holdout Samples}} for {{Model Selection}}},
  author = {Schorfheide, Frank and Wolpin, Kenneth I.},
  year = {2012},
  month = may,
  volume = {102},
  pages = {477--481},
  issn = {0002-8282},
  doi = {10.1257/aer.102.3.477},
  abstract = {Researchers often hold out data from the estimation of econometric models to use for external validation. However, the use of holdout samples is suboptimal from a Bayesian perspective, which prescribes using the entire sample to form posterior model weights. This paper examines a possible rationale for the use of holdout samples: data-inspired modifications of structural models are likely to lead to an exaggeration of model fit. The use of holdout samples can, in principle, set an incentive for the modeler not to exaggerate model fit.},
  file = {/home/peterprescott/Zotero/storage/EAATPJZR/articles.html},
  journal = {American Economic Review},
  keywords = {and Selection,Model Evaluation,Validation},
  language = {en},
  number = {3}
}

@article{KAbadirMagnus2002,
  title = {Notation in Econometrics: A Proposal for a Standard},
  shorttitle = {Notation in Econometrics},
  author = {Abadir, Karim and Magnus, Jan},
  year = {2002},
  month = jun,
  volume = {5},
  pages = {76--90},
  issn = {1368-4221, 1368-423X},
  doi = {10.1111/1368-423X.t01-1-00074},
  abstract = {This paper proposes a standard for notation in econometrics. It presents a fully integrated and internally consistent framework for notation and abbreviations, which is as close as possible to existing common practice. The symbols used are instantly recognizable and interpretable, thus minimizing ambiguity and enhancing reading efficiency. The standard is designed in a flexible manner, thus allowing for future extensions.},
  file = {/home/peterprescott/Zotero/storage/34Q5C77S/Abadir and Magnus - 2002 - Notation in econometrics a proposal for a standar.pdf},
  journal = {The Econometrics Journal},
  language = {en},
  number = {1}
}

@inproceedings{LDabbishEtAl2012,
  title = {Social Coding in {{GitHub}}: Transparency and Collaboration in an Open Software Repository},
  shorttitle = {Social Coding in {{GitHub}}},
  booktitle = {Proceedings of the {{ACM}} 2012 Conference on {{Computer Supported Cooperative Work}}},
  author = {Dabbish, Laura and Stuart, Colleen and Tsay, Jason and Herbsleb, Jim},
  year = {2012},
  month = feb,
  pages = {1277--1286},
  publisher = {{Association for Computing Machinery}},
  address = {{Seattle, Washington, USA}},
  doi = {10.1145/2145204.2145396},
  abstract = {Social applications on the web let users track and follow the activities of a large number of others regardless of location or affiliation. There is a potential for this transparency to radically improve collaboration and learning in complex knowledge-based activities. Based on a series of in-depth interviews with central and peripheral GitHub users, we examined the value of transparency for large-scale distributed collaborations and communities of practice. We find that people make a surprisingly rich set of social inferences from the networked activity information in GitHub, such as inferring someone else's technical goals and vision when they edit code, or guessing which of several similar projects has the best chance of thriving in the long term. Users combine these inferences into effective strategies for coordinating work, advancing technical skills and managing their reputation.},
  file = {/home/peterprescott/Zotero/storage/E8MKSLAS/Dabbish et al. - 2012 - Social coding in GitHub transparency and collabor.pdf},
  isbn = {978-1-4503-1086-4},
  keywords = {awareness,collaboration,coordination,open source software development,social computing,transparency},
  series = {{{CSCW}} '12}
}

@article{RPeng2011,
  title = {Reproducible Research in Computational Science},
  author = {Peng, Roger D.},
  year = {2011},
  volume = {334},
  pages = {1226--1227},
  publisher = {{American Association for the Advancement of Science}},
  file = {/home/peterprescott/Zotero/storage/SR47WKQ3/Peng - 2011 - Reproducible research in computational science.pdf;/home/peterprescott/Zotero/storage/6KC9ANVY/1226.html},
  journal = {Science},
  number = {6060}
}

@article{SRaschka2018,
  title = {{{MLxtend}}: {{Providing}} Machine Learning and Data Science Utilities and Extensions to {{Python}}'s Scientific Computing Stack},
  shorttitle = {{{MLxtend}}},
  author = {Raschka, Sebastian},
  year = {2018},
  volume = {3},
  pages = {638},
  file = {/home/peterprescott/Zotero/storage/5NZ3QI7N/Raschka - 2018 - MLxtend Providing machine learning and data scien.pdf},
  journal = {Journal of open source software},
  number = {24}
}

@misc{Statsmodels2020,
  title = {Add {{Root Mean Square Percentage Error}} as Rmspe(): {{Pull Request}} \#6774},
  author = {Statsmodels},
  year = {2020},
  file = {/home/peterprescott/Zotero/storage/PFSWL6J7/6774.html},
  howpublished = {https://github.com/statsmodels/statsmodels/pull/6774},
  journal = {GitHub},
  language = {en}
}

@book{TOliphant2006,
  title = {Guide to {{NumPy}}},
  shorttitle = {Guide to {{NumPy}}},
  author = {Oliphant, T.E.},
  year = {2006},
  publisher = {{Trelgol}},
  address = {{Austin, Tex.}},
  abstract = {This is the second edition of Travis Oliphant's A Guide to NumPy originally published electronically in 2006. It is designed to be a reference that can be used by practitioners who are familiar with Python but want to learn more about NumPy and related tools. In this updated edition, new perspectives are shared as well as descriptions of new distributed processing tools in the ecosystem, and how Numba can be used to compile code using NumPy arrays. Travis Oliphant is the co-founder and CEO of Continuum Analytics. Continuum Analytics develops Anaconda, the leading modern open source analytics platform powered by Python. Travis, who is a passionate advocate of open source technology, has a Ph.D. from Mayo Clinic and B.S. and M.S. degrees in Mathematics and Electrical Engineering from Brigham Young University. Since 1997, he has worked extensively with Python for computational and data science. He was the primary creator of the NumPy package and founding contributor to the SciPy package. He was also a co-founder and past board member of NumFOCUS, a non-profit for reproducible and accessible science that supports the PyData stack. He also served on the board of the Python Software Foundation.},
  isbn = {978-1-5173-0007-4},
  language = {English}
}

@article{VCerqueiraEtAl2019,
  title = {Evaluating Time Series Forecasting Models: {{An}} Empirical Study on Performance Estimation Methods},
  shorttitle = {Evaluating Time Series Forecasting Models},
  author = {Cerqueira, Vitor and Torgo, Luis and Mozetic, Igor},
  year = {2019},
  month = may,
  abstract = {Performance estimation aims at estimating the loss that a predictive model will incur on unseen data. These procedures are part of the pipeline in every machine learning project and are used for assessing the overall generalisation ability of predictive models. In this paper we address the application of these methods to time series forecasting tasks. For independent and identically distributed data the most common approach is cross-validation. However, the dependency among observations in time series raises some caveats about the most appropriate way to estimate performance in this type of data and currently there is no settled way to do so. We compare different variants of cross-validation and of out-of-sample approaches using two case studies: One with 62 real-world time series and another with three synthetic time series. Results show noticeable differences in the performance estimation methods in the two scenarios. In particular, empirical experiments suggest that cross-validation approaches can be applied to stationary time series. However, in real-world scenarios, when different sources of non-stationary variation are at play, the most accurate estimates are produced by out-of-sample methods that preserve the temporal order of observations.},
  archivePrefix = {arXiv},
  eprint = {1905.11744},
  eprinttype = {arxiv},
  file = {/home/peterprescott/Zotero/storage/TP9XYCYL/Cerqueira et al. - 2019 - Evaluating time series forecasting models An empi.pdf;/home/peterprescott/Zotero/storage/PN85BFML/1905.html},
  journal = {arXiv:1905.11744 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{WMcKinney2010,
  title = {Data Structures for Statistical Computing in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {McKinney, Wes},
  year = {2010},
  volume = {445},
  pages = {51--56},
  publisher = {{Austin, TX}},
  file = {/home/peterprescott/Zotero/storage/UUE22MFF/McKinney - 2010 - Data structures for statistical computing in pytho.pdf}
}

@misc{zotero-2119,
  title = {Universal Functions (Ufunc) \textemdash{} {{NumPy}} v1.18 {{Manual}}},
  file = {/home/peterprescott/Zotero/storage/BGHHJLBT/ufuncs.html},
  howpublished = {https://numpy.org/doc/stable/reference/ufuncs.html}
}


