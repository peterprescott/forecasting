{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pandoc/draft.md','r') as f:\n",
    "    txt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap codechunks in <details> tags\n",
    "# wrap adjacent codechunks in single <details> tag\n",
    "# wrap output chunks (no language name) in <details open>\n",
    "# remove codechunk ``` ticks from table output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_at = txt.split('```')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = ''\n",
    "for i, text in enumerate(split_at):\n",
    "    if i % 2 == 0:\n",
    "        # start\n",
    "        total = total + text + 'startchunk```'\n",
    "    else:\n",
    "        # end\n",
    "        total = total + text + 'stopchunk```'\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = total.replace('startchunk```python','<details>\\n<summary>code</summary>\\n```python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "step2 = step1.replace('stopchunk```','stopchunk```\\n</details>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: Forecasting Future Sales\\n---\\n\\n## Introduction\\n\\nThe art of predicting the future is both magical and mundane. Without some confidence that we can understand the causal dynamics of the cosmos, all attempts at decision-making would be rendered null and void. But the non-linear and chaotic interactions between the universe\\'s varied forces of causation mean that forecasting the behaviour of complex systems in advance can be incredibly difficult [@ELorenz1972].\\n\\nIn this report I describe my attempt to use 942 days of sales data for 1,115 drug stores located across Germany to predict daily sales for the subsequent 6 weeks. For the sake of reproducible research [@RPeng2011], I include all my code in expandable chunks at the appropriate points throughout this text.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nimport pandas as pd\\nfrom pandas import Timestamp\\nimport calendar\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\nfrom sklearn.metrics import make_scorer\\nfrom sklearn.linear_model import LinearRegression\\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\\nfrom IPython.display import HTML\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n## avoid cluttering page with warnings\\nimport warnings; warnings.filterwarnings(\\'ignore\\')\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n## The Data\\n\\nThe first step in data analysis is data exploration. The data was given in the form of three CSV files: `stores.csv` contained information for each pseudonymised store regarding their `StoreType`, their `Assortment` of stock, some data about `Competition`, and the store\\'s involvement with an ongoing coupon campaign referred to as `Promo2`; `train.csv` contained daily `Sales` and `Customers` data, including also information as to whether the day was an official `Holiday`, whether the store was `Open` that day, and whether the store was running a store-specific `Promo`; `test.csv` contained identical features, but with `Sales` and `Customers` data removed, so that the forecast can be tested. \\n\\n### Parsing Dates\\n\\nPython\\'s `pandas` library [@WMcKinney2010] for dataframe manipulation provides a powerful set of tools for manipulating dates -- all we have to do is specify which columns should be parsed as such when we load our data. \\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ntraining_data = pd.read_csv(\\'../data/raw/train.csv\\', parse_dates=[\\'Date\\'])\\ntest_data = pd.read_csv(\\'../data/raw/test.csv\\', parse_dates=[\\'Date\\'])\\nstopchunk```\\n</details>\\n\\n\\n\\n\\nActually, that is not quite true; for non-standard datetime parsing, we need to use `pd.to_datetime()` after `pd.read_csv()`. This is the case with the dates given for when each store began  experiencing competition, and (separately) participating in `Promo2`.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nstore_data = pd.read_csv(\\'../data/raw/store.csv\\',\\n                        parse_dates={\\'CompetitionDate\\':[5,4],\\n                                     \\'Promo2Date\\':[8,7]})\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nstore_data[\\'CompetitionDate\\'] = pd.to_datetime(\\n    store_data.CompetitionDate.astype(str), \\n    format=\\'%Y %m\\',\\n    errors=\\'coerce\\')\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nstore_data[\\'Promo2Date\\'] = pd.to_datetime(\\n    ## we have to insert a dummy \\'dayofweek\\' value (ie. \\'1\\')\\n    ## ... or it won\\'t work\\n    store_data.Promo2Date.astype(str)+\\' 1\\',\\n    format=\\'%Y %W %w\\',\\n    errors=\\'coerce\\')\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n### Stores Data: Data Cleaning\\n\\nOn loading the Stores Data, it appeared that there were two unnamed columns in the table -- but this proved to be merely a stray space on the top row. \\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nwith open(\\'../data/raw/store.csv\\',\\'r\\') as f:\\n    txt = f.read().split(\\'\\\\n\\')\\nfor i, line in enumerate(txt):\\n    col_11_value = line.split(\\',\\')[-1]\\n    if len(col_11_value)>0:\\n        print(f\\'Line {i} had \"{col_11_value}\" in the final column.\\')\\nstopchunk```\\n</details>\\n\\nstartchunk```\\nLine 1 had \" \" in the final column.\\nstopchunk```\\n</details>\\n\\n\\n\\nThe imposter columns were therefore removed.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nstore_data.drop(columns=[\"Unnamed: 11\", \"Unnamed: 10\"], inplace=True)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n### Sales Data: Muddled Dates\\n\\nThe Sales Data is supposed to be divided by date into two sets: a training set that goes from the beginning of 2013 until the end of July 2015, and a test set that goes from the start of August 2015. However, inspection suggested both datasets strayed outside these bounds, apparently including data from December 2015. \\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nprint(f\\'The latest date in `train.csv` is {max(training_data.Date)}.\\\\n\\'\\n        + f\\'The latest date in `test.csv` is {max(test_data.Date)}.\\')\\nstopchunk```\\n</details>\\n\\nstartchunk```\\nThe latest date in `train.csv` is 2015-12-07 00:00:00.\\nThe latest date in `test.csv` is 2015-12-09 00:00:00.\\nstopchunk```\\n</details>\\n\\n\\n\\nThis was because for all dates where the date of the month was less than or equal to twelve, the day and the month had been reversed, which we can see since the data is otherwise in perfect chronological order (Fig.1). So the first step in data cleaning was iterating through the dates and unmuddling the month and day.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n## set number of stores\\nn = 1115\\n\\ndef disordered_muddle(date_series, future_first=True):\\n    \"\"\"Check whether a series of dates is disordered or just muddled\"\"\"\\n    disordered = []\\n    muddle = []\\n    dates = date_series\\n    different_dates = pd.Series(dates.unique())\\n    date = different_dates[0]\\n    for i, d in enumerate(different_dates[1:]):\\n        ## we expect the date\\'s dayofyear to decrease by one\\n        if d.dayofyear!=date.dayofyear-1:\\n            ## unless the year is changing\\n            if d.year!=date.year-1:\\n                try:\\n                    ## we check if the day and month are muddled\\n                    ## if d.day > 12 this will cause an Exception\\n                    unmuddle = Timestamp(d.year,d.day,d.month)\\n                    if unmuddle.dayofyear==date.dayofyear-1:\\n                        muddle.append(d)\\n                        d = unmuddle\\n                    elif unmuddle.year==date.year-1:\\n                        muddle.append(d)\\n                        d = unmuddle\\n                    else:\\n                        disordered.append(d)\\n                except:\\n                    disordered.append(d)\\n        date=d\\n    if len(disordered)==0 and len(muddle)==0:\\n        return False\\n    else:\\n        return disordered, muddle\\n    \\ndef unmuddle(date_series, muddled_values):\\n    \"\"\"Unmuddle dates where month and day have been confused\"\"\"\\n    date_correction = {}\\n\\n    for d in date_series:\\n        if d in muddled_values:\\n            date_correction[d] = Timestamp(d.year, d.day, d.month)\\n        else:\\n            date_correction[d] = Timestamp(d.year, d.month, d.day)\\n\\n    return date_series.map(date_correction)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndisordered, muddle = disordered_muddle(training_data.Date)\\n_, test_muddle = disordered_muddle(test_data[\\'Date\\'])\\n\\ntraining_data[\\'CorrectedDate\\'] = unmuddle(training_data[\\'Date\\'],muddle)\\ntest_data[\\'CorrectedDate\\'] = unmuddle(test_data[\\'Date\\'], test_muddle)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nplt.rcParams.update({\\'font.size\\': 15})\\nfig, ax = plt.subplots(figsize=(10,5))\\nax.plot(training_data.Date.unique(), label=\\'Muddled Dates\\',c=\\'green\\',ls=\\':\\')\\nax.plot(training_data.CorrectedDate.unique(), label=\\'Corrected Dates\\',c=\\'blue\\',ls=\\'--\\')\\nax.legend()\\nplt.xlim(0,len(training_data.Date.unique()))\\nplt.xlabel(\\'Order\\')\\nplt.ylabel(\\'Date\\')\\nfig.suptitle(\\'Muddled Dates\\', y=0)\\nfig.savefig(\\'../figures/muddled_dates.png\\')\\nplt.tight_layout()\\nplt.savefig(\\'../figures/muddledates.png\\')\\nstopchunk```\\n</details>\\n\\n![](../figures/draft_figure12_1.png)\\\\\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ntraining_data[\\'Date\\'] = training_data[\\'CorrectedDate\\']\\ntraining_data.drop(columns=[\\'CorrectedDate\\'],inplace=True)\\ntest_data[\\'Date\\'] = test_data[\\'CorrectedDate\\']\\ntest_data.drop(columns=[\\'CorrectedDate\\'],inplace=True)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n### Sales Data: Missing Dates\\n\\nOnce the dates had been unmuddled, it became clear that 180 stores were missing all dates from the second half of 2014 -- that is, from the 1st July to the 31st December. \\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nn = len(store_data)\\ndates_for_store = {}\\nstores = []\\n\\n## iterate over Stores and get set of dates for each\\nfor i in range(1, n+1):\\n    dates_for_store[i] = set(training_data.loc[training_data.Store==i].Date)\\n    ## I know from further analysis that 758 is the magic number\\n    ## ... this is for demonstration purposes\\n    if len(dates_for_store[i]) == 758:\\n        stores.append(i)\\n\\n## find missing dates by difference of set of all dates and non-missing dates\\nnon_missing_dates = set()\\nfor i in stores:\\n    non_missing_dates = non_missing_dates.union(dates_for_store[i])\\nall_dates = set(training_data[\\'Date\\'])\\nmissing_dates = all_dates.difference(non_missing_dates)\\n## show that there are indeed 758 missing dates\\nassert (len(missing_dates) == len(training_data.Date.unique()) - 758)\\n\\n## show that the missing dates are an unbroken daily range\\nstart = min(missing_dates)\\nstop = max(missing_dates)\\nmissing_range = set(pd.date_range(start,stop))\\nassert missing_dates == missing_range\\n\\nprint(f\"\"\"{len(stores)} stores are missing dates \\\\\\nfrom {start.strftime(\"%Y-%m-%d\")} to {stop.strftime(\"%Y-%m-%d\")}\"\"\")\\nstopchunk```\\n</details>\\n\\nstartchunk```\\n180 stores are missing dates from 2014-07-01 to 2014-12-31\\nstopchunk```\\n</details>\\n\\n\\n\\nThese 180 stores include stores of all three `Assortment` and all four `StoreType` levels -- there was no clear evidence in the data given of any systematic bias in those stores with missing data. \\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nstores_missing_dates = store_data.loc[store_data[\\'Store\\'].isin(stores)]\\nprint(f\"\"\"\\nStores missing data from second half of 2014 \\nhave `Assortment` values {stores_missing_dates[\\'Assortment\\'].unique()}\\nand `StoreType` values {stores_missing_dates[\"StoreType\"].unique()}\\n \"\"\")\\nstopchunk```\\n</details>\\n\\nstartchunk```\\n\\nStores missing data from second half of 2014\\nhave `Assortment` values [\\'a\\' \\'c\\' \\'b\\']\\nand `StoreType` values [\\'d\\' \\'a\\' \\'c\\' \\'b\\']\\nstopchunk```\\n</details>\\n\\n\\n\\n\\nAlthough we might note that the data does not include geographic information, so a likely scenario would be that they could all be from the same region, and that region\\'s results were incorrectly merged with the others -- in this case, the missing data is likely to cause some unavoidable systematic error. Whatever the reason, I decided to estimate the sales those stores would have had, if they had been open, and then use those estimates in training our final model.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nfor i in range(1,n+1):\\n    if len(dates_for_store[i]) != len(training_data[\\'Date\\'].unique()) \\\\\\n    and i not in stores:\\n        another = training_data.loc[training_data[\\'Store\\']==i]\\n        missing = all_dates.difference(set(pd.Series(another.Date.unique())))\\n        print(f\\'{i} is missing {missing}\\')\\nstopchunk```\\n</details>\\n\\nstartchunk```\\n988 is missing {Timestamp(\\'2013-01-01 00:00:00\\')}\\nstopchunk```\\n</details>\\n\\n\\n\\n\\nThere is also another store -- number `988` -- that is missing some data: in this case just for the single date at the start of the range, 1st January 2013. One missing date out of 942 need not worry us.\\n\\n\\n### Integrating Data\\n\\nHaving done an initial exploration of the datasets, we integrate them on the `Store` index number. In checking this was as expected -- an unbroken integer sequence from 1 to 1,1115 -- it became apparent that for some reason the `test.csv` dataset only includes data for 856 stores. \\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n## assert `Store` index is as expected -- \\n## ie. an unbroken sequence of increasing integers starting at 1\\nfor i in range(1,len(store_data)+1):\\n    assert(i==store_data.Store.iloc[i-1])\\n## assert set of Stores in `store_data` is same as set of Stores in `training_data`\\nassert set(store_data.Store) == set(training_data.Store)\\n## but not the same as the set of stores in `test_data`\\nassert set(store_data.Store) != set(test_data.Store)\\nprint(f\\'`test_data` only has data for {len(test_data.Store.unique())} stores\\')\\nstopchunk```\\n</details>\\n\\nstartchunk```\\n`test_data` only has data for 856 stores\\nstopchunk```\\n</details>\\n\\n\\n\\nThe other two (`training.csv` and `store.csv`) are complete, so this missing data need not concern us in developing a predictive model. \\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nintegrated = pd.merge(training_data,store_data,on=\\'Store\\')\\nintegrated_test = pd.merge(test_data,store_data,on=\\'Store\\')\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n## Methodology\\n\\n### General Approach\\n\\nHaving loaded, cleaned, and integrated our data, we are in a position to consider the business problem which we must address: predicting six weeks of daily sales data. \\n\\nWe do this by iteratively developing a series of predictive linear regression models, on a sample of the `training_data` from the interval $[t_{0},t_{1}]$, and then validating the models on a *hold-out sample* [@FSchorfheideWolpin2012] from the interval $(t_{1},t_{2}]$.\\n\\nIn general, given a store number $i$ and a date $t$, we want to be able to know the sales value $S(i,t)$. We suppose that $S$ is a combination of some predictable $\\\\hat{S}(i,t)$ and some unpredictable noise $\\\\varepsilon$, and we want to optimize our prediction against some measure of error $e$.\\n\\nSpecifically, we will try to minimize the Root Mean Square Percentage Error (henceforth, the RMSPE). The RMSPE is preferable to the absolute Mean Square Percentage Error because it is calibrated to the result under consideration - an error of say 100 would be much less significant if it is for a predicted value of 10,000 (an error of 1%), than for a predicted value of 10 (an error of 1000%). But to my surprise, the usually excellent Python statistical library `statsmodels` included a `rmse()` Root Mean Squared Error function in its collection of evaluation measures, but not a Percentage Error. So I wrote the function and made a Pull Request for it to be added to the library [@Statsmodels2020].\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef rmspe(y, y_hat, axis=0, zeros=np.nan):\\n    \"\"\"\\n    Root Mean Squared Percentage Error\\n    \"\"\"\\n    y_hat = np.asarray(y_hat)\\n    y = np.asarray(y)\\n    error = y - y_hat\\n    percentage_error = np.divide(error, y, out=np.full_like(error,zeros), where=y!=0)\\n    return np.nanmean(percentage_error**2, axis=axis) * 100\\nstopchunk```\\n</details>\\n\\n\\n\\nThe only challenge is dealing with cases where the actual value is zero, in which case evaluating a percentage error means we might find ourselves dividing by zero. Fortunately, Python\\'s numerical array library `numpy` [@TOliphant2006] allows us to specify explicitly and straightforwardly how to deal with specific cases.\\n\\n\\n### The Minimal Model\\n\\nWe begin with a simple visualisation of all the million or so data points (Figure 2) of `Sales` over time (that is, the specific `Date`) -- ignoring of course whatever points from when a store was not `Open`, since unsurprisingly the `Sales` value in all such cases is zero.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nfig, ax = plt.subplots(figsize=(15,10))\\nopened = integrated.loc[integrated.Open==1]\\nmean_sales_vector = np.full(len(opened.Date), opened.Sales.mean())\\nall_dates = pd.concat([opened.Date, test_data.Date])\\nextended_sales_vector = np.full(len(all_dates), opened.Sales.mean())\\nax.scatter(opened.Date, opened.Sales, alpha=0.002,c=\\'b\\',marker=\\'.\\',label=\\'Sales\\')\\nax.plot(all_dates, extended_sales_vector,label=\\'Mean Sales\\',c=\\'r\\',ls=\\'-.\\')\\nax.axvline(Timestamp(2014,1,1),c=\\'grey\\',ls=\\'--\\')\\nax.axvline(Timestamp(2015,1,1),c=\\'grey\\',ls=\\'--\\')\\nax.set_ylim(0,20000)\\nax.set_xlim(Timestamp(2013,1,1),Timestamp(2015,9,18))\\nax.set_ylabel(\\'Sales\\')\\nax.set_xlabel(\\'Date\\')\\nleg = ax.legend(loc=1)\\nfor lh in leg.legendHandles: \\n    lh.set_alpha(1)\\nax.tick_params(axis=\\'x\\', rotation=30)\\nfig.suptitle(f\\'Two and a half years of Sales Data for {len(store_data)} Stores\\',y=0)\\nfig.tight_layout()\\n\\nplt.savefig(\\'../figures/scatterdata.png\\')\\nstopchunk```\\n</details>\\n\\n![](../figures/draft_figure20_1.png)\\\\\\n\\n\\nAlthough there is a large amount of vertical variation within the data, the line is strikingly horizontal over time. This immediately suggests a simplest model, in which we predict the `Sales` of any given store on any day to be the mean of all known `Sales`.\\n\\n$$\\\\hat{S} = \\\\mu_{\\\\mathcal{S}}$$\\n\\nwhere $\\\\mathcal{S}$ is the set of known `Sales` values, $S_{it}$ is the particular known `Sales` value for a given `Store` $i$ on a `Date` $t$, and we write $\\\\mu_{\\\\mathcal{S}}$ for the mean of the set $\\\\mathcal{S}$, such that $$\\\\mu_{ \\\\mathcal{S} } = \\\\frac{1}{| \\\\mathcal{S}|} \\\\sum_{S_{it} \\\\in \\\\mathcal{S}} S_{it}$$\\n\\nNote that while I try to use the appropriate notation [@KAbadirMagnus2002], the mathematics is here intended primarily as an exposition of the code, which is pragmatically focussed on optimizing a validated error score, rather than establishing any rigorous causal claim against a null hypothesis.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nopened = integrated.loc[integrated.Open==1]\\nmean_sales_vector = np.full(len(opened.Sales), opened.Sales.mean())\\nprint(f\"\"\"RMSPE of mean sales values as predictor of `Sales`:\\n{rmspe(opened.Sales, mean_sales_vector)}\"\"\"\\nstopchunk```\\n</details>\\n\\nstartchunk```\\n  File \"<ipython-input-1-faca698920ba>\", line 4\\n    {rmspe(opened.Sales, mean_sales_vector)}\"\"\"\\n\\n^\\nSyntaxError: unexpected EOF while parsing\\nstopchunk```\\n</details>\\n\\n\\n\\nThis gives us a RMSPE slightly over 40%.\\n\\n### Personalized Models\\n\\nOf course, we need not blindly consider all stores aggregated together, when we have precise data for each individual store. This suggests a better model:\\n\\n$$\\\\hat{S}(i) = \\\\mu_{\\\\mathcal{S}_{i}}$$\\n\\nwhere $\\\\mathcal{S}_{i} = \\\\{ S_{it} : t \\\\in [t_{0}, t_{1}] \\\\} \\\\subset \\\\mathcal{S}$, \\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nmean_mapper = {}\\nfor i in range(1,1115):\\n    mean_mapper[i] = opened.loc[opened.Store==i].Sales.mean()\\nopened[\\'IndividualMeans\\'] = opened[\\'Store\\'].map(mean_mapper)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nprint(f\\'RMSPE = {rmspe(opened.Sales, opened.IndividualMeans)}\\')\\nstopchunk```\\n</details>\\n\\nstartchunk```\\nRMSPE = 15.503699843759986\\nstopchunk```\\n</details>\\n\\n\\n\\nAlready we have a model which gives us a RMSPE of 15.4%.\\n\\nProperly speaking we should validate this score on a hold-out sample before too much congratulating ourselves -- just because the overall aggregate mean of sales across all stores is stationary across time, there is no guarantee that the trend of any individual store will be so -- in which case our model will do somewhat worse on unseen data. But we will factor time effects into our model before going to the trouble of validating over a different time period.\\n\\n### Seasonality and Trend\\n\\nThe other thing that is apparent from Figure 2 is a significant and repeated rise in sales just before the end of each year. This makes sense, in terms of extra shopping before the Christmas holidays. And it suggests that we pay closer attention to the performance of a particular Store not just in terms of its trend, but also in terms of its cyclic seasonal averages.\\n\\nI therefore implemented two simple functions to find naive versions of seasonality and trend independently for each store: ; `find_trend()` uses ordinary least squares regression to find a line of best fit between a store\\'s `Sales` and the `Date`\\'s `DayNumber` since the beginning of 2013; `get_averages()` finds the mean sales for every `Month` of the year, as well as for every `DayOfWeek`.\\n\\nMathematically, we use OLS to fit\\n\\n$$ T_{i}(t) = \\\\alpha_{i} + \\\\beta_{i}t \\\\approx S(i,t)$$\\n\\nand we find\\n\\n$$M_{i}(t) = M_{im} = \\\\mu_{ \\\\mathcal{S}_{im}}$$ for $t \\\\in m$, where $m$ is the set of dates falling within a particular month, $\\\\mathcal{S}_{im} = \\\\{ S_{it} : t \\\\in m \\\\} \\\\subset \\\\mathcal{S}_{i}$, and $M_{im} \\\\in \\\\mathbb{R}$ is the monthly average for the store in question.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef find_trend(df):\\n    \"Find line-of-best-fit for \\'Sales ~ DayNumber\\'\"\\n    \\n    dataset = df\\n    ## find `DayNumber` for each Date\\n    dataset[\\'DayNumber\\'] = dataset.Date.dt.dayofyear \\\\\\n                    + (dataset.Date.dt.year - 2013)*365\\n    ## only consider days when store is open\\n    dataset_open = dataset.loc[dataset.Open==1]\\n    \\n    naive_trend = {}\\n    dataset[\\'NaiveTrend\\'] = 0\\n    \\n    ## iterate through Stores\\n    for i in dataset.Store.unique():\\n        naive_trend[i] = ols(formula=\\'Sales~DayNumber\\', \\n                        data=dataset_open.loc[dataset_open.Store==i]).fit()\\n        dataset[\\'NaiveTrend\\'] = np.where(dataset.Store==i,\\n                        dataset.DayNumber*naive_trend[i].params[\\'DayNumber\\'] \\\\\\n                            + naive_trend[i].params[\\'Intercept\\'],\\n                        dataset[\\'NaiveTrend\\'])\\n        \\n    \\n    return dataset, naive_trend\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef get_averages(df):\\n    \"\"\"Get average Sales for various timeframes\"\"\"\\n    \\n    dataset = df\\n    t = dataset.NaiveTrend / dataset.NaiveTrend.mean()\\n    dataset[\\'Month\\'] = dataset[\\'Date\\'].dt.month\\n    mapper = {}\\n    length = [\\'Month\\',\\'DayOfWeek\\']\\n    df_list = []\\n    df_store = {}\\n    \\n    ## iterate through Stores\\n    for store in dataset.Store.unique():\\n        df_store[store] = dataset.loc[dataset.Store==store]\\n        mapper[store] = {}\\n        \\n        ## iterate through list of time periods\\n        for l in length:\\n            df_store[store][f\\'AverageFor{l}\\'] = 0\\n            mapper[store][l] = {}\\n            \\n            ## iterate over unique months/days of week\\n            for i in dataset[l].unique():\\n                ## get mean Sales value\\n                mapper[store][l][i] = df_store[store].loc[\\n                    df_store[store][l]==i][\\'Sales\\'].mean()\\n                df_store[store][f\\'AverageFor{l}\\'] = np.where(\\n                    df_store[store][l]==i,\\n                    df_store[store][l].map(mapper[store][l]),\\n                    df_store[store][f\\'AverageFor{l}\\'])\\n                \\n        D = df_store[store].AverageForDayOfWeek \\\\\\n                                    / df_store[store].AverageForDayOfWeek.mean()\\n        T = df_store[store].NaiveTrend / df_store[store].NaiveTrend.mean()\\n        df_store[store][\\'MTD\\'] = df_store[store].AverageForMonth * T * D\\n        df_list.append(df_store[store])\\n        \\n    dataset = pd.concat(df_list).sort_index()\\n    return dataset, mapper\\nstopchunk```\\n</details>\\n\\n\\n\\n\\nAs mentioned, 180 stores are missing data for the second half of 2014, so we begin by considering the eighteen months for which we have full data.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n\\neighteen_months = integrated.loc[integrated.Date <= Timestamp(2014,6,30)]\\neighteen_months = eighteen_months.loc[eighteen_months.Open==1]\\neighteen_months, _ = find_trend(eighteen_months)\\neighteen_months, _ = get_averages(eighteen_months)\\n\\nfig, axs = plt.subplots(2,2,figsize=(15,20))\\neg_stores = [302, 470, 756, 882]\\nfor i in range(4):\\n    j = i//2\\n    k = i%2\\n    ax = axs[j][k]\\n    eg_store = eg_stores[i]\\n    eg = eighteen_months.loc[eighteen_months.Store == eg_store].sort_index()\\n    t = eg.NaiveTrend/eg.NaiveTrend.mean()\\n    ax.plot(eg.Date,eg.NaiveTrend,c=\\'k\\',label=\\'NaiveTrend ~ T\\',ls=\\':\\')\\n    ax.plot(eg.Date,eg.AverageForMonth*t,label=\\'AverageForMonth * T\\',c=\\'b\\',ls=\\'--\\')\\n    ax.axvline(Timestamp(2014,1,1),c=\\'grey\\',ls=\\'--\\')\\n    ax.scatter(eg.Date, eg.Sales, alpha=0.3, c=\\'g\\', label=\\'Sales\\')\\n    ax.set_ylim(0,25000)\\n    ax.set_ylabel(\\'Sales\\')\\n    ax.set_xlim(Timestamp(2013,1,1),Timestamp(2014,6,30))\\n    ax.set_xlabel(\\'Date\\')\\n## https://stackoverflow.com/questions/12848808/set-legend-symbol-opacity-with-matplotlib\\n    leg = ax.legend()\\n    for lh in leg.legendHandles: \\n        lh.set_alpha(0.5)\\n    ax.set_title(f\\'Store {eg_store}\\')\\n    ax.tick_params(axis=\\'x\\', rotation=30)\\n    \\nfig.tight_layout()\\nplt.savefig(\\'../figures/trend_seasonality.png\\')\\nstopchunk```\\n</details>\\n\\n![](../figures/draft_figure26_1.png)\\\\\\n\\n\\nIn Figure 3 we see eighteen months of sales visualized for four different stores, as well as a grey dotted line showing the linear trend of those sales, and a blue dashed line showing the shifting seasonal monthly average of sales for those eighteen months. Actually, what is shown is not the average itself, but the product of the monthly average $M_{im}$ of sales for a store $i$ in a month $m$ with the normalized trend given by $\\\\bar{T_{i}}(t) = \\\\frac{T_{i}(t)}{\\\\mu_{\\\\mathcal{T}_{i}}}$.\\n\\nThus our model could be something like $$\\\\hat{S}(i,t) = M_{it}\\\\bar{T_{i}}(t)$$\\n\\nWe see this gives quite a good fit, and that our suspicion was justified that we could not rely on the trend of any particular store to be flat. But before we train and validate a model in these terms, we also visualize the effect of considering the effect of the average of sales for a store on a particular day of the week, $$D_{i}(t) = \\\\mu_{\\\\mathcal{S}_{id}}$$ where $t \\\\in d$, the set of all dates falling on some particular day of the week.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n\\nfig, axs = plt.subplots(4,2,figsize=(15,20))\\ndays = calendar.day_name\\neg_store = 274 #np.random.randint(1,n+1)\\neg = eighteen_months.loc[eighteen_months.Store == eg_store].sort_index()\\nfor i in range(0,8):\\n    j = i//2\\n    k = i%2\\n    ax = axs[j][k]\\n    if i == 0:\\n        df = eg\\n    else:\\n        df = eg.loc[eg.DayOfWeek == i]\\n    t = df.NaiveTrend/eg.NaiveTrend.mean()\\n    d = df.AverageForDayOfWeek/eg.AverageForDayOfWeek.mean()\\n    ax.scatter(df.Date, df.Sales, alpha=0.3, c=\\'red\\', label=\\'Sales\\')\\n    ax.axvline(Timestamp(2014,1,1),c=\\'grey\\',ls=\\'--\\')\\n    if i == 0:\\n        ax.plot(df.Date,df.AverageForMonth,label=\\'AverageForMonth = M\\',c=\\'purple\\',ls=\\'-\\')\\n        ax.plot(df.Date,df.NaiveTrend,label=\\'NaiveTrend ~ T\\', c=\\'k\\',ls=\\'--\\')\\n        ax.plot(df.Date,\\n                   df.AverageForDayOfWeek,\\n                   label=\\'AverageForDayOfWeek ~ D\\',\\n                   c=\\'brown\\',\\n                   ls=\\':\\',\\n                   marker=\\'.\\',\\n                   alpha=0.4)\\n    else:\\n        ax.plot(df.Date,df.AverageForMonth*t,label=\\'M * T\\',c=\\'purple\\',ls=\\':\\')\\n        ax.plot(df.Date,df.AverageForMonth*t*d,label=\\'M * T * D\\',c=\\'red\\',ls=\\'-.\\')\\n    ax.set_ylim(0,10000)\\n    ax.set_ylabel(\\'Sales\\')\\n    ax.set_xlim(Timestamp(2013,1,1),Timestamp(2014,6,30))\\n    ax.set_xlabel(\\'Date\\')\\n## https://stackoverflow.com/questions/12848808/set-legend-symbol-opacity-with-matplotlib\\n    leg = ax.legend()\\n    for lh in leg.legendHandles: \\n        lh.set_alpha(1)\\n    if i>0:\\n        ax.set_title(f\\'{days[i-1]}\\')\\n    else:\\n        ax.set_title(\\'All Days of the Week\\')\\n    ax.tick_params(axis=\\'x\\', rotation=30)\\n    \\nfig.suptitle(f\\'Sales by Day of Week for Store {eg_store}\\',y=0.01)\\nfig.tight_layout()\\n\\nplt.savefig(\\'../figures/daysofweek.png\\')\\nstopchunk```\\n</details>\\n\\n![](../figures/draft_figure27_1.png)\\\\\\n\\n\\nAgain, we use the normalized version $\\\\bar{D_{i}} = \\\\frac{D_{i}}{\\\\mu_{\\\\mathcal{D}_{i}}}$, and then consider the product of daily and monthly cyclic patterns with the ongoing trend, which because of its curves we will here call\\n\\n$$\\\\zeta(i,t) = M_{it}\\\\bar{T_{i}}\\\\bar{D_{i}}$$\\n\\n(In the code we simply add an extra feature to the dataframe named `MTD`).\\n\\nIt is clear from Figure 4 that also considering $\\\\bar{D_{i}}$ improves the fit of our model -- at least in the case $i = 274$.\\n\\n### Reproducible Code for Computing the Optimal Model\\n\\nThus far we have offered a visual suggestion that $\\\\hat{S} = \\\\zeta$ could be a good model, but nothing that could give any indication whether it might be an *optimal* model. To efficiently and persuasively compare and combine $\\\\zeta(i,t)$ with the various $\\\\delta_{F}(i,t) \\\\mapsto \\\\{ 0 , 1 \\\\}$ (where $\\\\delta_{F}$ is the binary mapping of a feature $F$ like `Promo` or `PublicHoliday`), we use the `Exhaustive Feature Selector` (imported as `EFS`) from the `mlxtend` library [@SRaschka2018] to suggest the best predictive models with 1, 2, and 3 (or indeed, however many we like) features.\\n\\n`EFS` performs a brute-force evaluation of all feature subsets, selecting the one which scores best again the given error metric (in our case, RMSPE).\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef suggest_best_predictors(fully_processed, max_features):\\n    \"\"\"\\n    Use Exhaustive Feature Selection \\n    to suggest best linear predictors \\n    for our fully processed dataset\\n    \"\"\"\\n    max_features = max_features\\n    efs, best = {}, {}\\n    best[\\'data\\'] = fully_processed\\n    lr = LinearRegression()\\n    rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\\n    for i in range(1, max_features+1):\\n        efs[i] = EFS(lr,\\n              min_features=i,\\n              max_features=i,\\n              scoring=rmspe_scorer, \\n              print_progress=False,\\n              cv=0\\n              )\\n        efs[i].fit(fully_processed.drop(columns=[\\'Sales\\']), \\n                   fully_processed[\\'Sales\\'], \\n                   custom_feature_names=fully_processed.drop(\\n                       columns=[\\'Sales\\']).columns)\\n        best[i] = efs[i].best_feature_names_\\n        \\n    keys = list(best.keys())\\n    keys.remove(\\'data\\')\\n    best[\\'max\\'] = max(keys)    \\n    return best\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef fit_models(best_predictors):\\n    \"\"\"\\n    Fit linear models and return results for given predictors.\\n    \"\"\"\\n    fully_processed = best_predictors[\\'data\\']\\n    \\n    max_features = best_predictors[\\'max\\']\\n    \\n    formula, lm, results, coefs, RMSPE, mapper, models = {}, {}, {}, {}, {}, {}, {}\\n\\n    for j in range(1, max_features+1):\\n        sum = \\'\\'\\n        for k, predictor in enumerate(best_predictors[j]):\\n            if k == 0:\\n                sum = f\\'Q(\"{predictor}\")\\'\\n            else:\\n                sum = f\\'{sum} + Q(\"{predictor}\")\\'\\n\\n        formula[j] = f\\'Q(\"Sales\") ~ {sum} \\'\\n\\n    for j in range(1, max_features+1):\\n\\n        lm[j] = ols(formula[j], fully_processed).fit()\\n        coefs[j] = lm[j].params\\n        RMSPE[j] = rmspe(fully_processed.Sales, lm[j].predict())\\n        \\n    results = pd.DataFrame(coefs)\\n\\n    for j in range(1, max_features+1):\\n        coefs[j] = lm[j].params\\n    results = pd.DataFrame(coefs).transpose()\\n    results[\\'RMSPE\\'] = pd.Series(RMSPE, \\n                                 index=list(range(1,max_features+1)), \\n                                 name=\\'RMSPE\\')\\n    results.index.name = \"Features\"\\n    \\n    for col in list(results.columns):\\n        if col[0]==\\'Q\\':\\n            mapper[col] = col[3:-2]\\n    results.rename(columns=mapper,inplace=True)\\n\\n    for i in range(len(results)):\\n        models[i+1] = dict(results.iloc[i])\\n        models[i+1].pop(\\'RMSPE\\')\\n        nan_keys = []\\n        for key in models[i+1].keys():\\n            if str(models[i+1][key]) == \\'nan\\':\\n                nan_keys.append(key)\\n        for key in nan_keys:\\n            models[i+1].pop(key)\\n    \\n    return models, results\\nstopchunk```\\n</details>\\n\\n\\n\\n\\nFirst though we need to preprocess our training sample: exclude dates where stores are closed, create dummy variables, and drop unnecessary columns.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef only_open(df):\\n    \"\"\"Only consider data from when store is open\"\"\"\\n    return df.loc[df.Open==1]\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef create_dummies(df):\\n    \"\"\"Create dummy variables\"\"\"\\n    \\n    dataset = df\\n    dataset[\\'PublicHoliday\\'] = np.where(dataset.StateHoliday == \\'a\\', 1, 0)\\n    dataset[\\'EasterHoliday\\'] = np.where(dataset.StateHoliday == \\'b\\', 1, 0)\\n    dataset[\\'ChristmasHoliday\\'] = np.where(dataset.StateHoliday == \\'c\\', 1, 0)\\n\\n    for x in dataset.StoreType.unique():\\n        dataset[f\\'Type{x.capitalize()}\\'] = np.where(dataset.StoreType == x, 1, 0)\\n\\n    for x in dataset.Assortment.unique():\\n        dataset[f\\'Assortment{x.capitalize()}\\'] = np.where(dataset.Assortment == x, 1, 0)\\n    \\n    return dataset\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef streamline(df, unnecessary_cols):\\n    \"\"\"Streamline df for ExhaustiveFeatureSelector by removing unnecessary columns\"\"\"\\n    return df.drop(columns=unnecessary_cols)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef prepare_training_set(start_date, end_date, unnecessary_cols):\\n    \"\"\"Prepare training set for given dates with single function\"\"\"\\n    dataset = integrated.loc[integrated.Date >= start_date]\\\\\\n                    .loc[integrated.Date <= end_date]\\n    dataset = only_open(dataset)\\n    dataset, trend = find_trend(dataset)\\n    dataset, mapper = get_averages(dataset)\\n    dataset = create_dummies(dataset)\\n    dataset = streamline(dataset, unnecessary_cols)\\n    return dataset, trend, mapper\\nstopchunk```\\n</details>\\n\\n\\n\\n\\nSimilarly we need to prepare the data for our validation dataset, making sure not to compute the averages and trend directly (thus invalidating our claim that we are forecasting the unknown future), but rather impute them on the assumption that they will be the same as those for our sampled training set.\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef extend_trend(naive_trend, validation_set):\\n    \"\"\"Extend (naively extracted) trend from training set to validation set\"\"\"\\n    validation_set[\\'DayNumber\\'] = validation_set.Date.dt.dayofyear \\\\\\n                    + (validation_set.Date.dt.year - 2013)*365\\n    validation_set[\\'NaiveTrend\\'] = 0\\n    for i in validation_set.Store.unique():\\n        validation_set[\\'NaiveTrend\\'] = np.where(validation_set.Store==i,\\n                                        validation_set.DayNumber*naive_trend[i].params[\\'DayNumber\\'] \\\\\\n                                         + naive_trend[i].params[\\'Intercept\\'],\\n                                         validation_set[\\'NaiveTrend\\'])\\n    return validation_set\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef impute_averages(validation_set, averages_mapper):\\n    \"\"\"Impute averages from training set to validation set\"\"\"\\n\\n    mapper = averages_mapper\\n    validation_set[\\'Month\\'] = validation_set[\\'Date\\'].dt.month\\n    \\n    length = [\\'Month\\',\\'DayOfWeek\\']\\n    df_list = []\\n    df_store = {}\\n    \\n    ## iterate through Stores\\n    for store in validation_set.Store.unique():\\n        df_store[store] = validation_set.loc[validation_set.Store==store]\\n        \\n        ## iterate through list of time periods\\n        for l in length:\\n            df_store[store][f\\'AverageFor{l}\\'] = 0\\n            \\n            ## iterate over unique months/days of week\\n            for i in validation_set[l].unique():\\n                ## get mean Sales value\\n                df_store[store][f\\'AverageFor{l}\\'] = np.where(\\n                    df_store[store][l]==i,\\n                    df_store[store][l].map(mapper[store][l]),\\n                    df_store[store][f\\'AverageFor{l}\\'])\\n                \\n        D = df_store[store].AverageForDayOfWeek \\\\\\n                                    / df_store[store].AverageForDayOfWeek.mean()\\n        T = df_store[store].NaiveTrend / df_store[store].NaiveTrend.mean()\\n        df_store[store][\\'MTD\\'] = df_store[store].AverageForMonth * T * D\\n        df_list.append(df_store[store])\\n        \\n    validation_set = pd.concat(df_list).sort_index()\\n    \\n    return(validation_set)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef predict_sales(model, row):\\n    \"\"\"Apply model to row to predict sales\"\"\"\\n    predictors = list(model.keys())\\n    predictors.remove(\\'Intercept\\')\\n    prediction = model[\\'Intercept\\']\\n    for p in predictors:\\n        prediction += row[p] * model[p]\\n    return prediction\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef prepare_validation_set(start_date, end_date, training_set, mapper, training_trend, unnecessary):\\n    \"\"\"Prepare validation dataset\"\"\"\\n    validation_set = integrated.loc[start_date <= integrated.Date]\\\\\\n                                .loc[integrated.Date <= end_date]\\n    validation_sales = validation_set.loc[validation_set[\\'Open\\']==1][\\'Sales\\']\\n    validation_set = validation_set.drop(columns=\\'Sales\\')\\n    validation_set = extend_trend(training_trend, validation_set)\\n    validation_set = impute_averages(validation_set, mapper)\\n    validation_set = validation_set.loc[validation_set[\\'Open\\']==1]\\n    validation_set = create_dummies(validation_set)\\n    validation_set = validation_set.drop(columns=unnecessary)   \\n    \\n    return validation_set, validation_sales\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndef validate_predictions(model, validation_set, validation_sales):\\n    \"\"\"Return RMSPE scores on validation data for prediction model\"\"\"\\n    predictions = validation_set.apply(lambda x: predict_sales(model, x),axis=1)\\n    return rmspe(validation_sales, predictions)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n\\ndef pandas_df_to_markdown_table(df):\\n    \"Round table figures and print as markdown table\"\\n\\n    df = df.round(3)\\n    df = df.reset_index()\\n    fmt = [\\'---\\' for i in range(len(df.columns))]\\n    df_fmt = pd.DataFrame([fmt], columns=df.columns)\\n    df_formatted = pd.concat([df_fmt, df])\\n    print(df_formatted.to_csv(sep=\"|\", index=False))\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n## Results\\n\\n### First Iteration: First Eighteen Months\\n\\nWe first train a model on the first eighteen months of data, and validate it on the following six months. If the validation confirms the adequacy of our model, then we can use our estimated values to fill in the six month gap for the 180 stores that we discovered in our data cleaning stage are missing data for those dates.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nunnecessary = [\\n    \\'Store\\',\\n    \\'Customers\\',\\n    \\'Open\\',\\n    \\'StateHoliday\\',\\n    \\'StoreType\\',\\n    \\'Assortment\\',\\n    \\'CompetitionDistance\\',\\n    \\'CompetitionDate\\',\\n    \\'Promo2Date\\',\\n    \\'PromoInterval\\',\\n    \\'Month\\',\\n    \\'DayOfWeek\\',\\n    \\'DayNumber\\',\\n    \\'NaiveTrend\\', \\n    \\'AverageForMonth\\', \\n    \\'AverageForDayOfWeek\\',\\n]\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nstart_date = Timestamp(2013,1,1)\\nend_date = Timestamp(2014,6,30)\\nprocessed_18, trend, mapper = prepare_training_set(start_date, end_date, unnecessary)\\nv_set, v_sales = prepare_validation_set(\\n    Timestamp(2014,7,1), Timestamp(2014,12,31), \\n    processed_18, mapper, trend, unnecessary)\\nprocessed_18 = streamline(processed_18, [\\'Date\\'])\\nbest = suggest_best_predictors(processed_18, 3)\\nmodels, results = fit_models(best)\\nv_score = {}\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nfor i in models.keys():\\n    v_score[i] = validate_predictions(models[i], v_set, v_sales)\\nresults[\\'Val. RMSPE\\'] = pd.DataFrame(v_score, index=[\\'Score\\']).T\\npandas_df_to_markdown_table(results)\\nstopchunk```\\n</details>\\n\\nstartchunk```\\nFeatures|Intercept|MTD|Promo|TypeB|RMSPE|Val. RMSPE\\n---|---|---|---|---|---|---\\n1|58.591|0.991|||7.443|13.754\\n2|-587.809|0.965|1914.375||5.473|9.538\\n3|-580.602|0.963|1916.183|232.116|5.465|9.518\\nstopchunk```\\n</details>\\n\\n\\n\\nOur method suggests three possible models, all of which seem to hold up on validation. \\n\\n### Second Iteration: Impute Estimates and Validate Further\\n\\nInspection of the code suggests that I chose the single-feature model, and the reason will be because of computational efficiency: rerunning the notebook as I worked on the analysis would be quicker with a simpler model.\\n\\nAlso, using the model to impute estimates for the missing six months is a non-trivial exercise. Before doing so, we must first generate the relevant data for those dates, since details like which days those 180 stores were open for during that time period, and whether or not they ran a `Promo` are all unknown.I estimated answers by saying that the pattern was identical as for the same day in the same week the previous year.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n## Impute model estimates for missing six months\\n\\n## generate dates\\nd_range = pd.date_range(Timestamp(2014,7,1),Timestamp(2014,12,31))\\n\\n## find out details (ie. holiday, day of week) of dates from those with data\\nsix_months = integrated.loc[integrated.Date >= Timestamp(2014,7,1)]\\\\\\n                    .loc[integrated.Date <= Timestamp(2014,12,31)]\\ndetails, info, df = {}, {}, {}\\nfor d in d_range:\\n    details[d] = {}\\n    details[d][\\'StateHoliday\\'] = six_months.loc[six_months.Date==d].StateHoliday.describe().top\\n    details[d][\\'SchoolHoliday\\'] = six_months.loc[six_months.Date==d].SchoolHoliday.median()\\n    details[d][\\'DayOfWeek\\'] = six_months.loc[six_months.Date==d].DayOfWeek.median()\\n\\n## we will assume that stores with no data for the second half of 2014,\\n## ... had exactly the same pattern of being `Open` and doing a `Promo` as they did the year before\\ni = 0\\nfor store in stores:\\n    info[store] = {}\\n    store_df = integrated.loc[integrated.Store == store]\\n    store_df = store_df.loc[store_df.Date.dt.year == 2013]\\n    for d in d_range:\\n        info[store][d] = {}\\n        day = d.dayofweek\\n        week = d.week\\n        last_year = store_df.loc[(\\n            store_df.Date.dt.week == week) & (store_df.Date.dt.dayofweek == day)]\\n        info[store][d][\\'Open\\'] = last_year.Open.values[0]\\n        info[store][d][\\'Promo\\'] = last_year.Promo.values[0]\\n        \\n        df[i] = {\\'Store\\': store,\\n                \\'Date\\': d,\\n                \\'Open\\': info[store][d][\\'Open\\'],\\n                \\'Promo\\': info[store][d][\\'Promo\\'],\\n                \\'StateHoliday\\': details[d][\\'StateHoliday\\'],\\n                \\'SchoolHoliday\\': details[d][\\'SchoolHoliday\\'],\\n                \\'DayOfWeek\\': details[d][\\'DayOfWeek\\']}\\n        i += 1\\n\\nestimates = pd.DataFrame(df).T\\nestimates = extend_trend(trend, estimates)\\nestimates = impute_averages(estimates, mapper)\\nestimates[\\'Sales\\'] = estimates.apply(lambda x: predict_sales(models[1], x), axis=1)\\nestimates[\\'Sales\\'] = np.where(estimates.Open==0, 0, estimates[\\'Sales\\'])\\n\\nstoretype, assortment, promo2 = {}, {}, {}\\nfor store in stores:\\n    storetype[store] = store_data.loc[store_data.Store == store].StoreType.values[0]\\n    assortment[store] = store_data.loc[store_data.Store == store].Assortment.values[0]\\n    promo2[store] = store_data.loc[store_data.Store == store].Promo2.values[0]\\n\\nestimates[\\'StoreType\\'] = estimates.Store.map(storetype)\\nestimates[\\'Assortment\\'] = estimates.Store.map(assortment)\\nestimates[\\'Promo2\\'] = estimates.Store.map(promo2)\\n\\nintegrated = pd.merge(integrated,estimates,how=\\'outer\\').sort_index()\\nstopchunk```\\n</details>\\n\\n\\n\\n\\nOnce the estimates have been imputed, we perform the second iteration of our method, this time training on all the (now synthetically complete) data from the first two years and three months, and validating on the subsequent three months.\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n## validate estimates\\nstart_date = Timestamp(2013,1,1)\\nend_date = Timestamp(2015,3,31)\\nprocessed_filled, trend, mapper = prepare_training_set(start_date, end_date, unnecessary)\\nv_set2, v_sales2 = prepare_validation_set(\\n    Timestamp(2015,4,1), Timestamp(2015,6,30), \\n    processed_filled, mapper, trend, unnecessary)\\nprocessed_filled = streamline(processed_filled, [\\'Date\\'])\\nbest = suggest_best_predictors(processed_filled, 3)\\nmodels, results2 = fit_models(best)\\nv_score = {} \\nfor i in models.keys():\\n    v_score[i] = validate_predictions(models[i], v_set2, v_sales2)\\nresults2[\\'Val. RMSPE\\'] = pd.DataFrame(v_score, index=[\\'Score\\']).T\\npandas_df_to_markdown_table(results2)\\nstopchunk```\\n</details>\\n\\nstartchunk```\\nFeatures|Intercept|MTD|Promo|TypeB|RMSPE|Val. RMSPE\\n---|---|---|---|---|---|---\\n1|57.769|0.991|||8.338|5.575\\n2|-584.408|0.967|1826.938||6.011|3.879\\n3|-576.754|0.965|1828.743|231.384|6.003|3.868\\nstopchunk```\\n</details>\\n\\n\\n\\nOur attempt to use estimated data to complete the set on which we train our models seems to have worked incredibly well. The unseen validation set now has an error of less than 5% if we take the two-variable model.\\n\\nIt does seem surprising that the RMSPE on the unseen validation data is significantly better than the RMSPE on the training data itself, but this can be explained by the fact that we are validating on data near the middle of the year (from the start of April through until the end of June, in this case), and much of the error will come from the end-of-year spike (as suggested by Figure 3). \\n\\n### Third Iteration: Hyper-Personalized Models\\n\\nAt this point we have a fairly good model:\\n\\n$$\\\\hat{S} = \\\\alpha + \\\\beta\\\\zeta_{i} + \\\\gamma\\\\delta_{P} \\\\pm e = S $$ \\n\\nwith $\\\\mathbb{E}(| e |) < \\\\frac{1}{10}S$.\\n\\nWe now improve it further, by leaning further in to the initial insight that we should consider each store individually. We might call this our *personalization thesis*.\\n\\nFor this final iteration of model development, we use `EFS` to find a best two-feature additive linear model for each store independently:\\n\\n$$\\\\Omega_{i} = \\\\alpha_{i} + \\\\beta_{i}f_{i} + \\\\gamma_{i}g_{i} \\\\pm e_{i} = S_{i} $$\\n\\nsuch that we minimize $\\\\mathbb{E}(| e_{i} |)$, with any two features $f_{i}, g_{i} \\\\in \\\\{ \\\\zeta_{i}, \\\\delta_{P}, \\\\delta_{P_{2}}, \\\\delta_{H_{P}}, \\\\delta_{H_{S}}, \\\\delta_{H_{E}}, \\\\delta_{H_{C}} \\\\}$.\\n\\nWe then create a new feature column `IndividualModel2` with the predicted values of that model for each date and store. And then we run `EFS` again, including all the stores together, to evaluate the errors and check whether a better score can be found by including any other feature with `IndividualModel2`.\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nstart, end = Timestamp(2013,1,1), Timestamp(2015,5,31)\\ntraining_set = integrated.loc[integrated.Date >= start]\\\\\\n                    .loc[integrated.Date <= end]\\nvalidation_set = integrated.loc[integrated.Date >= Timestamp(2015,6,1)]\\\\\\n                    .loc[integrated.Date <= Timestamp(2015,7,31)]\\ndf_store, vdf_store, trend_store = {}, {}, {}\\nbest_predictors, models_store, results_store = {}, {}, {}\\nprocessed_3, trend, mapper = prepare_training_set(start, end, unnecessary)\\nv_set3, v_sales3 = prepare_validation_set(\\n    Timestamp(2015,6,1), Timestamp(2015,7,31),\\n    processed_3, mapper, trend, unnecessary)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n## for each store we reduce the training_data and validation_set\\n## ...to that relevant to the store, and then do exhaustive feature selection\\n## ... to suggest the best predictors \\nprocessed_3.drop(columns=\\'Date\\', inplace=True)\\nfor store in range(1,n+1):\\n    df_store[store] = processed_3.loc[training_set.Store == store]\\n    vdf_store[store] = v_set3.loc[validation_set.Store == store]\\n    best_predictors[store] = suggest_best_predictors(df_store[store], 2)\\n    models_store[store], results_store[store] = fit_models(best_predictors[store])\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\npredictions = {}\\nfor store in training_set.Store.unique():\\n    for i in range(1,3):\\n        df_store[store][f\\'IndividualModel{i}\\'] = df_store[store].apply(\\n            lambda x: predict_sales(models_store[store][i], x), axis=1)\\n        vdf_store[store][f\\'IndividualModel{i}\\'] = vdf_store[store].apply(\\n            lambda x: predict_sales(models_store[store][i], x), axis=1)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\ndf_list, vdf_list = [], []\\nfor store in training_set.Store.unique():\\n    df_list.append(df_store[store])\\n    vdf_list.append(vdf_store[store])\\nind_training = pd.concat(df_list)\\nind_valid = pd.concat(vdf_list).drop(columns=\\'Date\\')\\n## v_set3.drop(columns=\\'Date\\', inplace=True)\\nfor i in range(1,3):\\n    processed_3[f\\'IndividualModel{i}\\'] = ind_training[f\\'IndividualModel{i}\\']\\n    v_set3[f\\'IndividualModel{i}\\'] = ind_valid[f\\'IndividualModel{i}\\']\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\n\\nbest3 = suggest_best_predictors(processed_3, 3)\\nmodels, results3 = fit_models(best3)\\nv_score = {} \\nfor i in models.keys():\\n    v_score[i] = validate_predictions(models[i], v_set3, v_sales3)\\nresults3[\\'Val. RMSPE\\'] = pd.DataFrame(v_score, index=[\\'Score\\']).T\\npandas_df_to_markdown_table(results3)\\nstopchunk```\\n</details>\\n\\nstartchunk```\\nFeatures|Intercept|EasterHoliday|IndividualModel2|Promo|RMSPE|Val.\\nRMSPE\\n---|---|---|---|---|---|---\\n1|0.0||1.0||4.803|2.566\\n2|0.382||0.998|23.864|4.798|2.558\\n3|0.356|-89.409|0.998|23.841|4.797|2.558\\nstopchunk```\\n</details>\\n\\n\\n\\nThis does indeed give improved model performance, and it seems likely our model would perform very well against the test data.\\n\\n## Interpretation and Conclusions\\n\\nMore interesting though than just an improved RMSPE score, is that we can now interrogate the details of each store\\'s best-performing `IndividualModel2` ($\\\\Omega_{i}$) to see what, if anything, the specific details may reveal. Indeed, our method has also yielded an `IndividualModel1`, a best single-feature model for each store. \\n\\nThis means we can see for each store which feature was the most powerful predictor, and which was second. We tabulate the aggregate scores in Table 4 (leaving out the two-feature case from any models which failed to improve the RMSPE score by more than 0.1, meaning the optimum model had already been found).\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\nimprovements = {}\\nfor store in range(1, n+1):\\n    improvements[store] = results_store[store].RMSPE[1]\\\\\\n                    - results_store[store].RMSPE[2]\\n\\nimprovement = pd.DataFrame(improvements, index= [\\'Improvement\\']).T\\nno_improvement = improvement.loc[improvement.Improvement<0.1].index\\n\\npredictor_scores = {}\\nfor i in range(1,3):\\n    predictor_scores[i] = {}\\nfor store in range(1, n+1):\\n    for i in range(1, 3):\\n        for p in best_predictors[store][i]:\\n            if not (store in no_improvement and i == 2):\\n                try:\\n                    predictor_scores[i][p] += 1\\n                except:\\n                    predictor_scores[i][p] = 1\\n\\nwith_one = pd.DataFrame(predictor_scores[1],index=[\\'OnePredictor\\']).T\\nwith_two = pd.DataFrame(predictor_scores[2],index=[\\'TwoPredictors\\']).T\\ncount_predictors = with_two.join(with_one)\\ncount_predictors.fillna(0,inplace=True)\\ncount_predictors[\\'Totals\\'] = count_predictors.OnePredictor \\\\\\n                                + count_predictors.TwoPredictors\\ncount_predictors = count_predictors.astype(int)\\nstopchunk```\\n</details>\\n\\n\\n\\n\\n\\n<details>\\n<summary>code</summary>\\n```python\\npandas_df_to_markdown_table(count_predictors.T)\\nstopchunk```\\n</details>\\n\\nstartchunk```\\nindex|Promo|MTD|SchoolHoliday|PublicHoliday|ChristmasHoliday|EasterHoliday\\n---|---|---|---|---|---|---\\nTwoPredictors|1020|1020|10|19|1|2\\nOnePredictor|755|359|0|1|0|0\\nTotals|1775|1379|10|20|1|2\\nstopchunk```\\n</details>\\n\\n\\n\\nTo our surprise, we find that in the `OnePredictor` case, for more than half of the stores the most powerful predictive feature was simply whether or not there was a `Promo` on that day -- rather than the `MTD` $\\\\zeta$ value we have calculated. \\n\\nWe conclude by noting that our models have been simple additive combinations of features -- although those features have included processed multiplicative products. If we needed to try and improve our model still further, we should include the product $\\\\zeta$ and $\\\\delta_{P}$, and perhaps also their products with `StateHoliday`. \\n\\nWe also note that because we treated each store individually, the general effect of features particular to stores is hidden: that is, we see `Promo` and the various types of `StateHoliday` showing up in our final analysis because they are features of a particular `Date`; and we do not see `StoreType` or `Assortment` because when a store is considered individually those features are constant. Also, we have not in the end considered the effect of `Promo2` and `CompetitionDistance`, since their effect is delayed in time and thus complex to detect.\\n\\n## References\\nstartchunk```'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---\\ntitle: Forecasting Future Sales\\n---\\n\\n## Introduction\\n\\nThe art of predicting the future is both magical and mundane. Without some confidence that we can understand the causal dynamics of the cosmos, all attempts at decision-making would be rendered null and void. But the non-linear and chaotic interactions between the universe\\'s varied forces of causation mean that forecasting the behaviour of complex systems in advance can be incredibly difficult [@ELorenz1972].\\n\\nIn this report I describe my attempt to use 942 days of sales data for 1,115 drug stores located across Germany to predict daily sales for the subsequent 6 weeks. For the sake of reproducible research [@RPeng2011], I include all my code in expandable chunks at the appropriate points throughout this text.\\n\\n\\n\\nstartchunk```python\\nimport pandas as pd\\nfrom pandas import Timestamp\\nimport calendar\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nimport statsmodels.api as sm\\nfrom statsmodels.formula.api import ols\\nfrom sklearn.metrics import make_scorer\\nfrom sklearn.linear_model import LinearRegression\\nfrom mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\\nfrom IPython.display import HTML\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\n## avoid cluttering page with warnings\\nimport warnings; warnings.filterwarnings(\\'ignore\\')\\nstopchunk```\\n\\n\\n\\n\\n## The Data\\n\\nThe first step in data analysis is data exploration. The data was given in the form of three CSV files: `stores.csv` contained information for each pseudonymised store regarding their `StoreType`, their `Assortment` of stock, some data about `Competition`, and the store\\'s involvement with an ongoing coupon campaign referred to as `Promo2`; `train.csv` contained daily `Sales` and `Customers` data, including also information as to whether the day was an official `Holiday`, whether the store was `Open` that day, and whether the store was running a store-specific `Promo`; `test.csv` contained identical features, but with `Sales` and `Customers` data removed, so that the forecast can be tested. \\n\\n### Parsing Dates\\n\\nPython\\'s `pandas` library [@WMcKinney2010] for dataframe manipulation provides a powerful set of tools for manipulating dates -- all we have to do is specify which columns should be parsed as such when we load our data. \\n\\n\\n\\nstartchunk```python\\ntraining_data = pd.read_csv(\\'../data/raw/train.csv\\', parse_dates=[\\'Date\\'])\\ntest_data = pd.read_csv(\\'../data/raw/test.csv\\', parse_dates=[\\'Date\\'])\\nstopchunk```\\n\\n\\n\\n\\nActually, that is not quite true; for non-standard datetime parsing, we need to use `pd.to_datetime()` after `pd.read_csv()`. This is the case with the dates given for when each store began  experiencing competition, and (separately) participating in `Promo2`.\\n\\n\\n\\nstartchunk```python\\nstore_data = pd.read_csv(\\'../data/raw/store.csv\\',\\n                        parse_dates={\\'CompetitionDate\\':[5,4],\\n                                     \\'Promo2Date\\':[8,7]})\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\nstore_data[\\'CompetitionDate\\'] = pd.to_datetime(\\n    store_data.CompetitionDate.astype(str), \\n    format=\\'%Y %m\\',\\n    errors=\\'coerce\\')\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\nstore_data[\\'Promo2Date\\'] = pd.to_datetime(\\n    ## we have to insert a dummy \\'dayofweek\\' value (ie. \\'1\\')\\n    ## ... or it won\\'t work\\n    store_data.Promo2Date.astype(str)+\\' 1\\',\\n    format=\\'%Y %W %w\\',\\n    errors=\\'coerce\\')\\nstopchunk```\\n\\n\\n\\n\\n### Stores Data: Data Cleaning\\n\\nOn loading the Stores Data, it appeared that there were two unnamed columns in the table -- but this proved to be merely a stray space on the top row. \\n\\n\\n\\n\\n\\nstartchunk```python\\nwith open(\\'../data/raw/store.csv\\',\\'r\\') as f:\\n    txt = f.read().split(\\'\\\\n\\')\\nfor i, line in enumerate(txt):\\n    col_11_value = line.split(\\',\\')[-1]\\n    if len(col_11_value)>0:\\n        print(f\\'Line {i} had \"{col_11_value}\" in the final column.\\')\\nstopchunk```\\n\\nstartchunk```\\nLine 1 had \" \" in the final column.\\nstopchunk```\\n\\n\\n\\nThe imposter columns were therefore removed.\\n\\n\\n\\nstartchunk```python\\nstore_data.drop(columns=[\"Unnamed: 11\", \"Unnamed: 10\"], inplace=True)\\nstopchunk```\\n\\n\\n\\n\\n### Sales Data: Muddled Dates\\n\\nThe Sales Data is supposed to be divided by date into two sets: a training set that goes from the beginning of 2013 until the end of July 2015, and a test set that goes from the start of August 2015. However, inspection suggested both datasets strayed outside these bounds, apparently including data from December 2015. \\n\\n\\n\\nstartchunk```python\\nprint(f\\'The latest date in `train.csv` is {max(training_data.Date)}.\\\\n\\'\\n        + f\\'The latest date in `test.csv` is {max(test_data.Date)}.\\')\\nstopchunk```\\n\\nstartchunk```\\nThe latest date in `train.csv` is 2015-12-07 00:00:00.\\nThe latest date in `test.csv` is 2015-12-09 00:00:00.\\nstopchunk```\\n\\n\\n\\nThis was because for all dates where the date of the month was less than or equal to twelve, the day and the month had been reversed, which we can see since the data is otherwise in perfect chronological order (Fig.1). So the first step in data cleaning was iterating through the dates and unmuddling the month and day.\\n\\n\\n\\nstartchunk```python\\n## set number of stores\\nn = 1115\\n\\ndef disordered_muddle(date_series, future_first=True):\\n    \"\"\"Check whether a series of dates is disordered or just muddled\"\"\"\\n    disordered = []\\n    muddle = []\\n    dates = date_series\\n    different_dates = pd.Series(dates.unique())\\n    date = different_dates[0]\\n    for i, d in enumerate(different_dates[1:]):\\n        ## we expect the date\\'s dayofyear to decrease by one\\n        if d.dayofyear!=date.dayofyear-1:\\n            ## unless the year is changing\\n            if d.year!=date.year-1:\\n                try:\\n                    ## we check if the day and month are muddled\\n                    ## if d.day > 12 this will cause an Exception\\n                    unmuddle = Timestamp(d.year,d.day,d.month)\\n                    if unmuddle.dayofyear==date.dayofyear-1:\\n                        muddle.append(d)\\n                        d = unmuddle\\n                    elif unmuddle.year==date.year-1:\\n                        muddle.append(d)\\n                        d = unmuddle\\n                    else:\\n                        disordered.append(d)\\n                except:\\n                    disordered.append(d)\\n        date=d\\n    if len(disordered)==0 and len(muddle)==0:\\n        return False\\n    else:\\n        return disordered, muddle\\n    \\ndef unmuddle(date_series, muddled_values):\\n    \"\"\"Unmuddle dates where month and day have been confused\"\"\"\\n    date_correction = {}\\n\\n    for d in date_series:\\n        if d in muddled_values:\\n            date_correction[d] = Timestamp(d.year, d.day, d.month)\\n        else:\\n            date_correction[d] = Timestamp(d.year, d.month, d.day)\\n\\n    return date_series.map(date_correction)\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\ndisordered, muddle = disordered_muddle(training_data.Date)\\n_, test_muddle = disordered_muddle(test_data[\\'Date\\'])\\n\\ntraining_data[\\'CorrectedDate\\'] = unmuddle(training_data[\\'Date\\'],muddle)\\ntest_data[\\'CorrectedDate\\'] = unmuddle(test_data[\\'Date\\'], test_muddle)\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\nplt.rcParams.update({\\'font.size\\': 15})\\nfig, ax = plt.subplots(figsize=(10,5))\\nax.plot(training_data.Date.unique(), label=\\'Muddled Dates\\',c=\\'green\\',ls=\\':\\')\\nax.plot(training_data.CorrectedDate.unique(), label=\\'Corrected Dates\\',c=\\'blue\\',ls=\\'--\\')\\nax.legend()\\nplt.xlim(0,len(training_data.Date.unique()))\\nplt.xlabel(\\'Order\\')\\nplt.ylabel(\\'Date\\')\\nfig.suptitle(\\'Muddled Dates\\', y=0)\\nfig.savefig(\\'../figures/muddled_dates.png\\')\\nplt.tight_layout()\\nplt.savefig(\\'../figures/muddledates.png\\')\\nstopchunk```\\n\\n![](../figures/draft_figure12_1.png)\\\\\\n\\n\\n\\n\\nstartchunk```python\\ntraining_data[\\'Date\\'] = training_data[\\'CorrectedDate\\']\\ntraining_data.drop(columns=[\\'CorrectedDate\\'],inplace=True)\\ntest_data[\\'Date\\'] = test_data[\\'CorrectedDate\\']\\ntest_data.drop(columns=[\\'CorrectedDate\\'],inplace=True)\\nstopchunk```\\n\\n\\n\\n\\n### Sales Data: Missing Dates\\n\\nOnce the dates had been unmuddled, it became clear that 180 stores were missing all dates from the second half of 2014 -- that is, from the 1st July to the 31st December. \\n\\n\\n\\nstartchunk```python\\nn = len(store_data)\\ndates_for_store = {}\\nstores = []\\n\\n## iterate over Stores and get set of dates for each\\nfor i in range(1, n+1):\\n    dates_for_store[i] = set(training_data.loc[training_data.Store==i].Date)\\n    ## I know from further analysis that 758 is the magic number\\n    ## ... this is for demonstration purposes\\n    if len(dates_for_store[i]) == 758:\\n        stores.append(i)\\n\\n## find missing dates by difference of set of all dates and non-missing dates\\nnon_missing_dates = set()\\nfor i in stores:\\n    non_missing_dates = non_missing_dates.union(dates_for_store[i])\\nall_dates = set(training_data[\\'Date\\'])\\nmissing_dates = all_dates.difference(non_missing_dates)\\n## show that there are indeed 758 missing dates\\nassert (len(missing_dates) == len(training_data.Date.unique()) - 758)\\n\\n## show that the missing dates are an unbroken daily range\\nstart = min(missing_dates)\\nstop = max(missing_dates)\\nmissing_range = set(pd.date_range(start,stop))\\nassert missing_dates == missing_range\\n\\nprint(f\"\"\"{len(stores)} stores are missing dates \\\\\\nfrom {start.strftime(\"%Y-%m-%d\")} to {stop.strftime(\"%Y-%m-%d\")}\"\"\")\\nstopchunk```\\n\\nstartchunk```\\n180 stores are missing dates from 2014-07-01 to 2014-12-31\\nstopchunk```\\n\\n\\n\\nThese 180 stores include stores of all three `Assortment` and all four `StoreType` levels -- there was no clear evidence in the data given of any systematic bias in those stores with missing data. \\n\\n\\n\\nstartchunk```python\\nstores_missing_dates = store_data.loc[store_data[\\'Store\\'].isin(stores)]\\nprint(f\"\"\"\\nStores missing data from second half of 2014 \\nhave `Assortment` values {stores_missing_dates[\\'Assortment\\'].unique()}\\nand `StoreType` values {stores_missing_dates[\"StoreType\"].unique()}\\n \"\"\")\\nstopchunk```\\n\\nstartchunk```\\n\\nStores missing data from second half of 2014\\nhave `Assortment` values [\\'a\\' \\'c\\' \\'b\\']\\nand `StoreType` values [\\'d\\' \\'a\\' \\'c\\' \\'b\\']\\nstopchunk```\\n\\n\\n\\n\\nAlthough we might note that the data does not include geographic information, so a likely scenario would be that they could all be from the same region, and that region\\'s results were incorrectly merged with the others -- in this case, the missing data is likely to cause some unavoidable systematic error. Whatever the reason, I decided to estimate the sales those stores would have had, if they had been open, and then use those estimates in training our final model.\\n\\n\\n\\nstartchunk```python\\nfor i in range(1,n+1):\\n    if len(dates_for_store[i]) != len(training_data[\\'Date\\'].unique()) \\\\\\n    and i not in stores:\\n        another = training_data.loc[training_data[\\'Store\\']==i]\\n        missing = all_dates.difference(set(pd.Series(another.Date.unique())))\\n        print(f\\'{i} is missing {missing}\\')\\nstopchunk```\\n\\nstartchunk```\\n988 is missing {Timestamp(\\'2013-01-01 00:00:00\\')}\\nstopchunk```\\n\\n\\n\\n\\nThere is also another store -- number `988` -- that is missing some data: in this case just for the single date at the start of the range, 1st January 2013. One missing date out of 942 need not worry us.\\n\\n\\n### Integrating Data\\n\\nHaving done an initial exploration of the datasets, we integrate them on the `Store` index number. In checking this was as expected -- an unbroken integer sequence from 1 to 1,1115 -- it became apparent that for some reason the `test.csv` dataset only includes data for 856 stores. \\n\\n\\n\\nstartchunk```python\\n## assert `Store` index is as expected -- \\n## ie. an unbroken sequence of increasing integers starting at 1\\nfor i in range(1,len(store_data)+1):\\n    assert(i==store_data.Store.iloc[i-1])\\n## assert set of Stores in `store_data` is same as set of Stores in `training_data`\\nassert set(store_data.Store) == set(training_data.Store)\\n## but not the same as the set of stores in `test_data`\\nassert set(store_data.Store) != set(test_data.Store)\\nprint(f\\'`test_data` only has data for {len(test_data.Store.unique())} stores\\')\\nstopchunk```\\n\\nstartchunk```\\n`test_data` only has data for 856 stores\\nstopchunk```\\n\\n\\n\\nThe other two (`training.csv` and `store.csv`) are complete, so this missing data need not concern us in developing a predictive model. \\n\\n\\n\\nstartchunk```python\\nintegrated = pd.merge(training_data,store_data,on=\\'Store\\')\\nintegrated_test = pd.merge(test_data,store_data,on=\\'Store\\')\\nstopchunk```\\n\\n\\n\\n\\n## Methodology\\n\\n### General Approach\\n\\nHaving loaded, cleaned, and integrated our data, we are in a position to consider the business problem which we must address: predicting six weeks of daily sales data. \\n\\nWe do this by iteratively developing a series of predictive linear regression models, on a sample of the `training_data` from the interval $[t_{0},t_{1}]$, and then validating the models on a *hold-out sample* [@FSchorfheideWolpin2012] from the interval $(t_{1},t_{2}]$.\\n\\nIn general, given a store number $i$ and a date $t$, we want to be able to know the sales value $S(i,t)$. We suppose that $S$ is a combination of some predictable $\\\\hat{S}(i,t)$ and some unpredictable noise $\\\\varepsilon$, and we want to optimize our prediction against some measure of error $e$.\\n\\nSpecifically, we will try to minimize the Root Mean Square Percentage Error (henceforth, the RMSPE). The RMSPE is preferable to the absolute Mean Square Percentage Error because it is calibrated to the result under consideration - an error of say 100 would be much less significant if it is for a predicted value of 10,000 (an error of 1%), than for a predicted value of 10 (an error of 1000%). But to my surprise, the usually excellent Python statistical library `statsmodels` included a `rmse()` Root Mean Squared Error function in its collection of evaluation measures, but not a Percentage Error. So I wrote the function and made a Pull Request for it to be added to the library [@Statsmodels2020].\\n\\n\\nstartchunk```python\\ndef rmspe(y, y_hat, axis=0, zeros=np.nan):\\n    \"\"\"\\n    Root Mean Squared Percentage Error\\n    \"\"\"\\n    y_hat = np.asarray(y_hat)\\n    y = np.asarray(y)\\n    error = y - y_hat\\n    percentage_error = np.divide(error, y, out=np.full_like(error,zeros), where=y!=0)\\n    return np.nanmean(percentage_error**2, axis=axis) * 100\\nstopchunk```\\n\\n\\n\\nThe only challenge is dealing with cases where the actual value is zero, in which case evaluating a percentage error means we might find ourselves dividing by zero. Fortunately, Python\\'s numerical array library `numpy` [@TOliphant2006] allows us to specify explicitly and straightforwardly how to deal with specific cases.\\n\\n\\n### The Minimal Model\\n\\nWe begin with a simple visualisation of all the million or so data points (Figure 2) of `Sales` over time (that is, the specific `Date`) -- ignoring of course whatever points from when a store was not `Open`, since unsurprisingly the `Sales` value in all such cases is zero.\\n\\n\\n\\nstartchunk```python\\nfig, ax = plt.subplots(figsize=(15,10))\\nopened = integrated.loc[integrated.Open==1]\\nmean_sales_vector = np.full(len(opened.Date), opened.Sales.mean())\\nall_dates = pd.concat([opened.Date, test_data.Date])\\nextended_sales_vector = np.full(len(all_dates), opened.Sales.mean())\\nax.scatter(opened.Date, opened.Sales, alpha=0.002,c=\\'b\\',marker=\\'.\\',label=\\'Sales\\')\\nax.plot(all_dates, extended_sales_vector,label=\\'Mean Sales\\',c=\\'r\\',ls=\\'-.\\')\\nax.axvline(Timestamp(2014,1,1),c=\\'grey\\',ls=\\'--\\')\\nax.axvline(Timestamp(2015,1,1),c=\\'grey\\',ls=\\'--\\')\\nax.set_ylim(0,20000)\\nax.set_xlim(Timestamp(2013,1,1),Timestamp(2015,9,18))\\nax.set_ylabel(\\'Sales\\')\\nax.set_xlabel(\\'Date\\')\\nleg = ax.legend(loc=1)\\nfor lh in leg.legendHandles: \\n    lh.set_alpha(1)\\nax.tick_params(axis=\\'x\\', rotation=30)\\nfig.suptitle(f\\'Two and a half years of Sales Data for {len(store_data)} Stores\\',y=0)\\nfig.tight_layout()\\n\\nplt.savefig(\\'../figures/scatterdata.png\\')\\nstopchunk```\\n\\n![](../figures/draft_figure20_1.png)\\\\\\n\\n\\nAlthough there is a large amount of vertical variation within the data, the line is strikingly horizontal over time. This immediately suggests a simplest model, in which we predict the `Sales` of any given store on any day to be the mean of all known `Sales`.\\n\\n$$\\\\hat{S} = \\\\mu_{\\\\mathcal{S}}$$\\n\\nwhere $\\\\mathcal{S}$ is the set of known `Sales` values, $S_{it}$ is the particular known `Sales` value for a given `Store` $i$ on a `Date` $t$, and we write $\\\\mu_{\\\\mathcal{S}}$ for the mean of the set $\\\\mathcal{S}$, such that $$\\\\mu_{ \\\\mathcal{S} } = \\\\frac{1}{| \\\\mathcal{S}|} \\\\sum_{S_{it} \\\\in \\\\mathcal{S}} S_{it}$$\\n\\nNote that while I try to use the appropriate notation [@KAbadirMagnus2002], the mathematics is here intended primarily as an exposition of the code, which is pragmatically focussed on optimizing a validated error score, rather than establishing any rigorous causal claim against a null hypothesis.\\n\\n\\n\\nstartchunk```python\\nopened = integrated.loc[integrated.Open==1]\\nmean_sales_vector = np.full(len(opened.Sales), opened.Sales.mean())\\nprint(f\"\"\"RMSPE of mean sales values as predictor of `Sales`:\\n{rmspe(opened.Sales, mean_sales_vector)}\"\"\"\\nstopchunk```\\n\\nstartchunk```\\n  File \"<ipython-input-1-faca698920ba>\", line 4\\n    {rmspe(opened.Sales, mean_sales_vector)}\"\"\"\\n\\n^\\nSyntaxError: unexpected EOF while parsing\\nstopchunk```\\n\\n\\n\\nThis gives us a RMSPE slightly over 40%.\\n\\n### Personalized Models\\n\\nOf course, we need not blindly consider all stores aggregated together, when we have precise data for each individual store. This suggests a better model:\\n\\n$$\\\\hat{S}(i) = \\\\mu_{\\\\mathcal{S}_{i}}$$\\n\\nwhere $\\\\mathcal{S}_{i} = \\\\{ S_{it} : t \\\\in [t_{0}, t_{1}] \\\\} \\\\subset \\\\mathcal{S}$, \\n\\n\\n\\nstartchunk```python\\nmean_mapper = {}\\nfor i in range(1,1115):\\n    mean_mapper[i] = opened.loc[opened.Store==i].Sales.mean()\\nopened[\\'IndividualMeans\\'] = opened[\\'Store\\'].map(mean_mapper)\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\nprint(f\\'RMSPE = {rmspe(opened.Sales, opened.IndividualMeans)}\\')\\nstopchunk```\\n\\nstartchunk```\\nRMSPE = 15.503699843759986\\nstopchunk```\\n\\n\\n\\nAlready we have a model which gives us a RMSPE of 15.4%.\\n\\nProperly speaking we should validate this score on a hold-out sample before too much congratulating ourselves -- just because the overall aggregate mean of sales across all stores is stationary across time, there is no guarantee that the trend of any individual store will be so -- in which case our model will do somewhat worse on unseen data. But we will factor time effects into our model before going to the trouble of validating over a different time period.\\n\\n### Seasonality and Trend\\n\\nThe other thing that is apparent from Figure 2 is a significant and repeated rise in sales just before the end of each year. This makes sense, in terms of extra shopping before the Christmas holidays. And it suggests that we pay closer attention to the performance of a particular Store not just in terms of its trend, but also in terms of its cyclic seasonal averages.\\n\\nI therefore implemented two simple functions to find naive versions of seasonality and trend independently for each store: ; `find_trend()` uses ordinary least squares regression to find a line of best fit between a store\\'s `Sales` and the `Date`\\'s `DayNumber` since the beginning of 2013; `get_averages()` finds the mean sales for every `Month` of the year, as well as for every `DayOfWeek`.\\n\\nMathematically, we use OLS to fit\\n\\n$$ T_{i}(t) = \\\\alpha_{i} + \\\\beta_{i}t \\\\approx S(i,t)$$\\n\\nand we find\\n\\n$$M_{i}(t) = M_{im} = \\\\mu_{ \\\\mathcal{S}_{im}}$$ for $t \\\\in m$, where $m$ is the set of dates falling within a particular month, $\\\\mathcal{S}_{im} = \\\\{ S_{it} : t \\\\in m \\\\} \\\\subset \\\\mathcal{S}_{i}$, and $M_{im} \\\\in \\\\mathbb{R}$ is the monthly average for the store in question.\\n\\n\\n\\nstartchunk```python\\ndef find_trend(df):\\n    \"Find line-of-best-fit for \\'Sales ~ DayNumber\\'\"\\n    \\n    dataset = df\\n    ## find `DayNumber` for each Date\\n    dataset[\\'DayNumber\\'] = dataset.Date.dt.dayofyear \\\\\\n                    + (dataset.Date.dt.year - 2013)*365\\n    ## only consider days when store is open\\n    dataset_open = dataset.loc[dataset.Open==1]\\n    \\n    naive_trend = {}\\n    dataset[\\'NaiveTrend\\'] = 0\\n    \\n    ## iterate through Stores\\n    for i in dataset.Store.unique():\\n        naive_trend[i] = ols(formula=\\'Sales~DayNumber\\', \\n                        data=dataset_open.loc[dataset_open.Store==i]).fit()\\n        dataset[\\'NaiveTrend\\'] = np.where(dataset.Store==i,\\n                        dataset.DayNumber*naive_trend[i].params[\\'DayNumber\\'] \\\\\\n                            + naive_trend[i].params[\\'Intercept\\'],\\n                        dataset[\\'NaiveTrend\\'])\\n        \\n    \\n    return dataset, naive_trend\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\ndef get_averages(df):\\n    \"\"\"Get average Sales for various timeframes\"\"\"\\n    \\n    dataset = df\\n    t = dataset.NaiveTrend / dataset.NaiveTrend.mean()\\n    dataset[\\'Month\\'] = dataset[\\'Date\\'].dt.month\\n    mapper = {}\\n    length = [\\'Month\\',\\'DayOfWeek\\']\\n    df_list = []\\n    df_store = {}\\n    \\n    ## iterate through Stores\\n    for store in dataset.Store.unique():\\n        df_store[store] = dataset.loc[dataset.Store==store]\\n        mapper[store] = {}\\n        \\n        ## iterate through list of time periods\\n        for l in length:\\n            df_store[store][f\\'AverageFor{l}\\'] = 0\\n            mapper[store][l] = {}\\n            \\n            ## iterate over unique months/days of week\\n            for i in dataset[l].unique():\\n                ## get mean Sales value\\n                mapper[store][l][i] = df_store[store].loc[\\n                    df_store[store][l]==i][\\'Sales\\'].mean()\\n                df_store[store][f\\'AverageFor{l}\\'] = np.where(\\n                    df_store[store][l]==i,\\n                    df_store[store][l].map(mapper[store][l]),\\n                    df_store[store][f\\'AverageFor{l}\\'])\\n                \\n        D = df_store[store].AverageForDayOfWeek \\\\\\n                                    / df_store[store].AverageForDayOfWeek.mean()\\n        T = df_store[store].NaiveTrend / df_store[store].NaiveTrend.mean()\\n        df_store[store][\\'MTD\\'] = df_store[store].AverageForMonth * T * D\\n        df_list.append(df_store[store])\\n        \\n    dataset = pd.concat(df_list).sort_index()\\n    return dataset, mapper\\nstopchunk```\\n\\n\\n\\n\\nAs mentioned, 180 stores are missing data for the second half of 2014, so we begin by considering the eighteen months for which we have full data.\\n\\n\\n\\nstartchunk```python\\n\\neighteen_months = integrated.loc[integrated.Date <= Timestamp(2014,6,30)]\\neighteen_months = eighteen_months.loc[eighteen_months.Open==1]\\neighteen_months, _ = find_trend(eighteen_months)\\neighteen_months, _ = get_averages(eighteen_months)\\n\\nfig, axs = plt.subplots(2,2,figsize=(15,20))\\neg_stores = [302, 470, 756, 882]\\nfor i in range(4):\\n    j = i//2\\n    k = i%2\\n    ax = axs[j][k]\\n    eg_store = eg_stores[i]\\n    eg = eighteen_months.loc[eighteen_months.Store == eg_store].sort_index()\\n    t = eg.NaiveTrend/eg.NaiveTrend.mean()\\n    ax.plot(eg.Date,eg.NaiveTrend,c=\\'k\\',label=\\'NaiveTrend ~ T\\',ls=\\':\\')\\n    ax.plot(eg.Date,eg.AverageForMonth*t,label=\\'AverageForMonth * T\\',c=\\'b\\',ls=\\'--\\')\\n    ax.axvline(Timestamp(2014,1,1),c=\\'grey\\',ls=\\'--\\')\\n    ax.scatter(eg.Date, eg.Sales, alpha=0.3, c=\\'g\\', label=\\'Sales\\')\\n    ax.set_ylim(0,25000)\\n    ax.set_ylabel(\\'Sales\\')\\n    ax.set_xlim(Timestamp(2013,1,1),Timestamp(2014,6,30))\\n    ax.set_xlabel(\\'Date\\')\\n## https://stackoverflow.com/questions/12848808/set-legend-symbol-opacity-with-matplotlib\\n    leg = ax.legend()\\n    for lh in leg.legendHandles: \\n        lh.set_alpha(0.5)\\n    ax.set_title(f\\'Store {eg_store}\\')\\n    ax.tick_params(axis=\\'x\\', rotation=30)\\n    \\nfig.tight_layout()\\nplt.savefig(\\'../figures/trend_seasonality.png\\')\\nstopchunk```\\n\\n![](../figures/draft_figure26_1.png)\\\\\\n\\n\\nIn Figure 3 we see eighteen months of sales visualized for four different stores, as well as a grey dotted line showing the linear trend of those sales, and a blue dashed line showing the shifting seasonal monthly average of sales for those eighteen months. Actually, what is shown is not the average itself, but the product of the monthly average $M_{im}$ of sales for a store $i$ in a month $m$ with the normalized trend given by $\\\\bar{T_{i}}(t) = \\\\frac{T_{i}(t)}{\\\\mu_{\\\\mathcal{T}_{i}}}$.\\n\\nThus our model could be something like $$\\\\hat{S}(i,t) = M_{it}\\\\bar{T_{i}}(t)$$\\n\\nWe see this gives quite a good fit, and that our suspicion was justified that we could not rely on the trend of any particular store to be flat. But before we train and validate a model in these terms, we also visualize the effect of considering the effect of the average of sales for a store on a particular day of the week, $$D_{i}(t) = \\\\mu_{\\\\mathcal{S}_{id}}$$ where $t \\\\in d$, the set of all dates falling on some particular day of the week.\\n\\n\\n\\nstartchunk```python\\n\\nfig, axs = plt.subplots(4,2,figsize=(15,20))\\ndays = calendar.day_name\\neg_store = 274 #np.random.randint(1,n+1)\\neg = eighteen_months.loc[eighteen_months.Store == eg_store].sort_index()\\nfor i in range(0,8):\\n    j = i//2\\n    k = i%2\\n    ax = axs[j][k]\\n    if i == 0:\\n        df = eg\\n    else:\\n        df = eg.loc[eg.DayOfWeek == i]\\n    t = df.NaiveTrend/eg.NaiveTrend.mean()\\n    d = df.AverageForDayOfWeek/eg.AverageForDayOfWeek.mean()\\n    ax.scatter(df.Date, df.Sales, alpha=0.3, c=\\'red\\', label=\\'Sales\\')\\n    ax.axvline(Timestamp(2014,1,1),c=\\'grey\\',ls=\\'--\\')\\n    if i == 0:\\n        ax.plot(df.Date,df.AverageForMonth,label=\\'AverageForMonth = M\\',c=\\'purple\\',ls=\\'-\\')\\n        ax.plot(df.Date,df.NaiveTrend,label=\\'NaiveTrend ~ T\\', c=\\'k\\',ls=\\'--\\')\\n        ax.plot(df.Date,\\n                   df.AverageForDayOfWeek,\\n                   label=\\'AverageForDayOfWeek ~ D\\',\\n                   c=\\'brown\\',\\n                   ls=\\':\\',\\n                   marker=\\'.\\',\\n                   alpha=0.4)\\n    else:\\n        ax.plot(df.Date,df.AverageForMonth*t,label=\\'M * T\\',c=\\'purple\\',ls=\\':\\')\\n        ax.plot(df.Date,df.AverageForMonth*t*d,label=\\'M * T * D\\',c=\\'red\\',ls=\\'-.\\')\\n    ax.set_ylim(0,10000)\\n    ax.set_ylabel(\\'Sales\\')\\n    ax.set_xlim(Timestamp(2013,1,1),Timestamp(2014,6,30))\\n    ax.set_xlabel(\\'Date\\')\\n## https://stackoverflow.com/questions/12848808/set-legend-symbol-opacity-with-matplotlib\\n    leg = ax.legend()\\n    for lh in leg.legendHandles: \\n        lh.set_alpha(1)\\n    if i>0:\\n        ax.set_title(f\\'{days[i-1]}\\')\\n    else:\\n        ax.set_title(\\'All Days of the Week\\')\\n    ax.tick_params(axis=\\'x\\', rotation=30)\\n    \\nfig.suptitle(f\\'Sales by Day of Week for Store {eg_store}\\',y=0.01)\\nfig.tight_layout()\\n\\nplt.savefig(\\'../figures/daysofweek.png\\')\\nstopchunk```\\n\\n![](../figures/draft_figure27_1.png)\\\\\\n\\n\\nAgain, we use the normalized version $\\\\bar{D_{i}} = \\\\frac{D_{i}}{\\\\mu_{\\\\mathcal{D}_{i}}}$, and then consider the product of daily and monthly cyclic patterns with the ongoing trend, which because of its curves we will here call\\n\\n$$\\\\zeta(i,t) = M_{it}\\\\bar{T_{i}}\\\\bar{D_{i}}$$\\n\\n(In the code we simply add an extra feature to the dataframe named `MTD`).\\n\\nIt is clear from Figure 4 that also considering $\\\\bar{D_{i}}$ improves the fit of our model -- at least in the case $i = 274$.\\n\\n### Reproducible Code for Computing the Optimal Model\\n\\nThus far we have offered a visual suggestion that $\\\\hat{S} = \\\\zeta$ could be a good model, but nothing that could give any indication whether it might be an *optimal* model. To efficiently and persuasively compare and combine $\\\\zeta(i,t)$ with the various $\\\\delta_{F}(i,t) \\\\mapsto \\\\{ 0 , 1 \\\\}$ (where $\\\\delta_{F}$ is the binary mapping of a feature $F$ like `Promo` or `PublicHoliday`), we use the `Exhaustive Feature Selector` (imported as `EFS`) from the `mlxtend` library [@SRaschka2018] to suggest the best predictive models with 1, 2, and 3 (or indeed, however many we like) features.\\n\\n`EFS` performs a brute-force evaluation of all feature subsets, selecting the one which scores best again the given error metric (in our case, RMSPE).\\n\\n\\n\\n\\nstartchunk```python\\ndef suggest_best_predictors(fully_processed, max_features):\\n    \"\"\"\\n    Use Exhaustive Feature Selection \\n    to suggest best linear predictors \\n    for our fully processed dataset\\n    \"\"\"\\n    max_features = max_features\\n    efs, best = {}, {}\\n    best[\\'data\\'] = fully_processed\\n    lr = LinearRegression()\\n    rmspe_scorer = make_scorer(rmspe, greater_is_better=False)\\n    for i in range(1, max_features+1):\\n        efs[i] = EFS(lr,\\n              min_features=i,\\n              max_features=i,\\n              scoring=rmspe_scorer, \\n              print_progress=False,\\n              cv=0\\n              )\\n        efs[i].fit(fully_processed.drop(columns=[\\'Sales\\']), \\n                   fully_processed[\\'Sales\\'], \\n                   custom_feature_names=fully_processed.drop(\\n                       columns=[\\'Sales\\']).columns)\\n        best[i] = efs[i].best_feature_names_\\n        \\n    keys = list(best.keys())\\n    keys.remove(\\'data\\')\\n    best[\\'max\\'] = max(keys)    \\n    return best\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\ndef fit_models(best_predictors):\\n    \"\"\"\\n    Fit linear models and return results for given predictors.\\n    \"\"\"\\n    fully_processed = best_predictors[\\'data\\']\\n    \\n    max_features = best_predictors[\\'max\\']\\n    \\n    formula, lm, results, coefs, RMSPE, mapper, models = {}, {}, {}, {}, {}, {}, {}\\n\\n    for j in range(1, max_features+1):\\n        sum = \\'\\'\\n        for k, predictor in enumerate(best_predictors[j]):\\n            if k == 0:\\n                sum = f\\'Q(\"{predictor}\")\\'\\n            else:\\n                sum = f\\'{sum} + Q(\"{predictor}\")\\'\\n\\n        formula[j] = f\\'Q(\"Sales\") ~ {sum} \\'\\n\\n    for j in range(1, max_features+1):\\n\\n        lm[j] = ols(formula[j], fully_processed).fit()\\n        coefs[j] = lm[j].params\\n        RMSPE[j] = rmspe(fully_processed.Sales, lm[j].predict())\\n        \\n    results = pd.DataFrame(coefs)\\n\\n    for j in range(1, max_features+1):\\n        coefs[j] = lm[j].params\\n    results = pd.DataFrame(coefs).transpose()\\n    results[\\'RMSPE\\'] = pd.Series(RMSPE, \\n                                 index=list(range(1,max_features+1)), \\n                                 name=\\'RMSPE\\')\\n    results.index.name = \"Features\"\\n    \\n    for col in list(results.columns):\\n        if col[0]==\\'Q\\':\\n            mapper[col] = col[3:-2]\\n    results.rename(columns=mapper,inplace=True)\\n\\n    for i in range(len(results)):\\n        models[i+1] = dict(results.iloc[i])\\n        models[i+1].pop(\\'RMSPE\\')\\n        nan_keys = []\\n        for key in models[i+1].keys():\\n            if str(models[i+1][key]) == \\'nan\\':\\n                nan_keys.append(key)\\n        for key in nan_keys:\\n            models[i+1].pop(key)\\n    \\n    return models, results\\nstopchunk```\\n\\n\\n\\n\\nFirst though we need to preprocess our training sample: exclude dates where stores are closed, create dummy variables, and drop unnecessary columns.\\n\\n\\n\\nstartchunk```python\\ndef only_open(df):\\n    \"\"\"Only consider data from when store is open\"\"\"\\n    return df.loc[df.Open==1]\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\ndef create_dummies(df):\\n    \"\"\"Create dummy variables\"\"\"\\n    \\n    dataset = df\\n    dataset[\\'PublicHoliday\\'] = np.where(dataset.StateHoliday == \\'a\\', 1, 0)\\n    dataset[\\'EasterHoliday\\'] = np.where(dataset.StateHoliday == \\'b\\', 1, 0)\\n    dataset[\\'ChristmasHoliday\\'] = np.where(dataset.StateHoliday == \\'c\\', 1, 0)\\n\\n    for x in dataset.StoreType.unique():\\n        dataset[f\\'Type{x.capitalize()}\\'] = np.where(dataset.StoreType == x, 1, 0)\\n\\n    for x in dataset.Assortment.unique():\\n        dataset[f\\'Assortment{x.capitalize()}\\'] = np.where(dataset.Assortment == x, 1, 0)\\n    \\n    return dataset\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\ndef streamline(df, unnecessary_cols):\\n    \"\"\"Streamline df for ExhaustiveFeatureSelector by removing unnecessary columns\"\"\"\\n    return df.drop(columns=unnecessary_cols)\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\ndef prepare_training_set(start_date, end_date, unnecessary_cols):\\n    \"\"\"Prepare training set for given dates with single function\"\"\"\\n    dataset = integrated.loc[integrated.Date >= start_date]\\\\\\n                    .loc[integrated.Date <= end_date]\\n    dataset = only_open(dataset)\\n    dataset, trend = find_trend(dataset)\\n    dataset, mapper = get_averages(dataset)\\n    dataset = create_dummies(dataset)\\n    dataset = streamline(dataset, unnecessary_cols)\\n    return dataset, trend, mapper\\nstopchunk```\\n\\n\\n\\n\\nSimilarly we need to prepare the data for our validation dataset, making sure not to compute the averages and trend directly (thus invalidating our claim that we are forecasting the unknown future), but rather impute them on the assumption that they will be the same as those for our sampled training set.\\n\\n\\n\\n\\nstartchunk```python\\ndef extend_trend(naive_trend, validation_set):\\n    \"\"\"Extend (naively extracted) trend from training set to validation set\"\"\"\\n    validation_set[\\'DayNumber\\'] = validation_set.Date.dt.dayofyear \\\\\\n                    + (validation_set.Date.dt.year - 2013)*365\\n    validation_set[\\'NaiveTrend\\'] = 0\\n    for i in validation_set.Store.unique():\\n        validation_set[\\'NaiveTrend\\'] = np.where(validation_set.Store==i,\\n                                        validation_set.DayNumber*naive_trend[i].params[\\'DayNumber\\'] \\\\\\n                                         + naive_trend[i].params[\\'Intercept\\'],\\n                                         validation_set[\\'NaiveTrend\\'])\\n    return validation_set\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\ndef impute_averages(validation_set, averages_mapper):\\n    \"\"\"Impute averages from training set to validation set\"\"\"\\n\\n    mapper = averages_mapper\\n    validation_set[\\'Month\\'] = validation_set[\\'Date\\'].dt.month\\n    \\n    length = [\\'Month\\',\\'DayOfWeek\\']\\n    df_list = []\\n    df_store = {}\\n    \\n    ## iterate through Stores\\n    for store in validation_set.Store.unique():\\n        df_store[store] = validation_set.loc[validation_set.Store==store]\\n        \\n        ## iterate through list of time periods\\n        for l in length:\\n            df_store[store][f\\'AverageFor{l}\\'] = 0\\n            \\n            ## iterate over unique months/days of week\\n            for i in validation_set[l].unique():\\n                ## get mean Sales value\\n                df_store[store][f\\'AverageFor{l}\\'] = np.where(\\n                    df_store[store][l]==i,\\n                    df_store[store][l].map(mapper[store][l]),\\n                    df_store[store][f\\'AverageFor{l}\\'])\\n                \\n        D = df_store[store].AverageForDayOfWeek \\\\\\n                                    / df_store[store].AverageForDayOfWeek.mean()\\n        T = df_store[store].NaiveTrend / df_store[store].NaiveTrend.mean()\\n        df_store[store][\\'MTD\\'] = df_store[store].AverageForMonth * T * D\\n        df_list.append(df_store[store])\\n        \\n    validation_set = pd.concat(df_list).sort_index()\\n    \\n    return(validation_set)\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\ndef predict_sales(model, row):\\n    \"\"\"Apply model to row to predict sales\"\"\"\\n    predictors = list(model.keys())\\n    predictors.remove(\\'Intercept\\')\\n    prediction = model[\\'Intercept\\']\\n    for p in predictors:\\n        prediction += row[p] * model[p]\\n    return prediction\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\ndef prepare_validation_set(start_date, end_date, training_set, mapper, training_trend, unnecessary):\\n    \"\"\"Prepare validation dataset\"\"\"\\n    validation_set = integrated.loc[start_date <= integrated.Date]\\\\\\n                                .loc[integrated.Date <= end_date]\\n    validation_sales = validation_set.loc[validation_set[\\'Open\\']==1][\\'Sales\\']\\n    validation_set = validation_set.drop(columns=\\'Sales\\')\\n    validation_set = extend_trend(training_trend, validation_set)\\n    validation_set = impute_averages(validation_set, mapper)\\n    validation_set = validation_set.loc[validation_set[\\'Open\\']==1]\\n    validation_set = create_dummies(validation_set)\\n    validation_set = validation_set.drop(columns=unnecessary)   \\n    \\n    return validation_set, validation_sales\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\ndef validate_predictions(model, validation_set, validation_sales):\\n    \"\"\"Return RMSPE scores on validation data for prediction model\"\"\"\\n    predictions = validation_set.apply(lambda x: predict_sales(model, x),axis=1)\\n    return rmspe(validation_sales, predictions)\\nstopchunk```\\n\\n\\n\\n\\nstartchunk```python\\n\\ndef pandas_df_to_markdown_table(df):\\n    \"Round table figures and print as markdown table\"\\n\\n    df = df.round(3)\\n    df = df.reset_index()\\n    fmt = [\\'---\\' for i in range(len(df.columns))]\\n    df_fmt = pd.DataFrame([fmt], columns=df.columns)\\n    df_formatted = pd.concat([df_fmt, df])\\n    print(df_formatted.to_csv(sep=\"|\", index=False))\\nstopchunk```\\n\\n\\n\\n\\n## Results\\n\\n### First Iteration: First Eighteen Months\\n\\nWe first train a model on the first eighteen months of data, and validate it on the following six months. If the validation confirms the adequacy of our model, then we can use our estimated values to fill in the six month gap for the 180 stores that we discovered in our data cleaning stage are missing data for those dates.\\n\\n\\n\\nstartchunk```python\\nunnecessary = [\\n    \\'Store\\',\\n    \\'Customers\\',\\n    \\'Open\\',\\n    \\'StateHoliday\\',\\n    \\'StoreType\\',\\n    \\'Assortment\\',\\n    \\'CompetitionDistance\\',\\n    \\'CompetitionDate\\',\\n    \\'Promo2Date\\',\\n    \\'PromoInterval\\',\\n    \\'Month\\',\\n    \\'DayOfWeek\\',\\n    \\'DayNumber\\',\\n    \\'NaiveTrend\\', \\n    \\'AverageForMonth\\', \\n    \\'AverageForDayOfWeek\\',\\n]\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\nstart_date = Timestamp(2013,1,1)\\nend_date = Timestamp(2014,6,30)\\nprocessed_18, trend, mapper = prepare_training_set(start_date, end_date, unnecessary)\\nv_set, v_sales = prepare_validation_set(\\n    Timestamp(2014,7,1), Timestamp(2014,12,31), \\n    processed_18, mapper, trend, unnecessary)\\nprocessed_18 = streamline(processed_18, [\\'Date\\'])\\nbest = suggest_best_predictors(processed_18, 3)\\nmodels, results = fit_models(best)\\nv_score = {}\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\nfor i in models.keys():\\n    v_score[i] = validate_predictions(models[i], v_set, v_sales)\\nresults[\\'Val. RMSPE\\'] = pd.DataFrame(v_score, index=[\\'Score\\']).T\\npandas_df_to_markdown_table(results)\\nstopchunk```\\n\\nstartchunk```\\nFeatures|Intercept|MTD|Promo|TypeB|RMSPE|Val. RMSPE\\n---|---|---|---|---|---|---\\n1|58.591|0.991|||7.443|13.754\\n2|-587.809|0.965|1914.375||5.473|9.538\\n3|-580.602|0.963|1916.183|232.116|5.465|9.518\\nstopchunk```\\n\\n\\n\\nOur method suggests three possible models, all of which seem to hold up on validation. \\n\\n### Second Iteration: Impute Estimates and Validate Further\\n\\nInspection of the code suggests that I chose the single-feature model, and the reason will be because of computational efficiency: rerunning the notebook as I worked on the analysis would be quicker with a simpler model.\\n\\nAlso, using the model to impute estimates for the missing six months is a non-trivial exercise. Before doing so, we must first generate the relevant data for those dates, since details like which days those 180 stores were open for during that time period, and whether or not they ran a `Promo` are all unknown.I estimated answers by saying that the pattern was identical as for the same day in the same week the previous year.\\n\\n\\n\\nstartchunk```python\\n## Impute model estimates for missing six months\\n\\n## generate dates\\nd_range = pd.date_range(Timestamp(2014,7,1),Timestamp(2014,12,31))\\n\\n## find out details (ie. holiday, day of week) of dates from those with data\\nsix_months = integrated.loc[integrated.Date >= Timestamp(2014,7,1)]\\\\\\n                    .loc[integrated.Date <= Timestamp(2014,12,31)]\\ndetails, info, df = {}, {}, {}\\nfor d in d_range:\\n    details[d] = {}\\n    details[d][\\'StateHoliday\\'] = six_months.loc[six_months.Date==d].StateHoliday.describe().top\\n    details[d][\\'SchoolHoliday\\'] = six_months.loc[six_months.Date==d].SchoolHoliday.median()\\n    details[d][\\'DayOfWeek\\'] = six_months.loc[six_months.Date==d].DayOfWeek.median()\\n\\n## we will assume that stores with no data for the second half of 2014,\\n## ... had exactly the same pattern of being `Open` and doing a `Promo` as they did the year before\\ni = 0\\nfor store in stores:\\n    info[store] = {}\\n    store_df = integrated.loc[integrated.Store == store]\\n    store_df = store_df.loc[store_df.Date.dt.year == 2013]\\n    for d in d_range:\\n        info[store][d] = {}\\n        day = d.dayofweek\\n        week = d.week\\n        last_year = store_df.loc[(\\n            store_df.Date.dt.week == week) & (store_df.Date.dt.dayofweek == day)]\\n        info[store][d][\\'Open\\'] = last_year.Open.values[0]\\n        info[store][d][\\'Promo\\'] = last_year.Promo.values[0]\\n        \\n        df[i] = {\\'Store\\': store,\\n                \\'Date\\': d,\\n                \\'Open\\': info[store][d][\\'Open\\'],\\n                \\'Promo\\': info[store][d][\\'Promo\\'],\\n                \\'StateHoliday\\': details[d][\\'StateHoliday\\'],\\n                \\'SchoolHoliday\\': details[d][\\'SchoolHoliday\\'],\\n                \\'DayOfWeek\\': details[d][\\'DayOfWeek\\']}\\n        i += 1\\n\\nestimates = pd.DataFrame(df).T\\nestimates = extend_trend(trend, estimates)\\nestimates = impute_averages(estimates, mapper)\\nestimates[\\'Sales\\'] = estimates.apply(lambda x: predict_sales(models[1], x), axis=1)\\nestimates[\\'Sales\\'] = np.where(estimates.Open==0, 0, estimates[\\'Sales\\'])\\n\\nstoretype, assortment, promo2 = {}, {}, {}\\nfor store in stores:\\n    storetype[store] = store_data.loc[store_data.Store == store].StoreType.values[0]\\n    assortment[store] = store_data.loc[store_data.Store == store].Assortment.values[0]\\n    promo2[store] = store_data.loc[store_data.Store == store].Promo2.values[0]\\n\\nestimates[\\'StoreType\\'] = estimates.Store.map(storetype)\\nestimates[\\'Assortment\\'] = estimates.Store.map(assortment)\\nestimates[\\'Promo2\\'] = estimates.Store.map(promo2)\\n\\nintegrated = pd.merge(integrated,estimates,how=\\'outer\\').sort_index()\\nstopchunk```\\n\\n\\n\\n\\nOnce the estimates have been imputed, we perform the second iteration of our method, this time training on all the (now synthetically complete) data from the first two years and three months, and validating on the subsequent three months.\\n\\n\\n\\nstartchunk```python\\n## validate estimates\\nstart_date = Timestamp(2013,1,1)\\nend_date = Timestamp(2015,3,31)\\nprocessed_filled, trend, mapper = prepare_training_set(start_date, end_date, unnecessary)\\nv_set2, v_sales2 = prepare_validation_set(\\n    Timestamp(2015,4,1), Timestamp(2015,6,30), \\n    processed_filled, mapper, trend, unnecessary)\\nprocessed_filled = streamline(processed_filled, [\\'Date\\'])\\nbest = suggest_best_predictors(processed_filled, 3)\\nmodels, results2 = fit_models(best)\\nv_score = {} \\nfor i in models.keys():\\n    v_score[i] = validate_predictions(models[i], v_set2, v_sales2)\\nresults2[\\'Val. RMSPE\\'] = pd.DataFrame(v_score, index=[\\'Score\\']).T\\npandas_df_to_markdown_table(results2)\\nstopchunk```\\n\\nstartchunk```\\nFeatures|Intercept|MTD|Promo|TypeB|RMSPE|Val. RMSPE\\n---|---|---|---|---|---|---\\n1|57.769|0.991|||8.338|5.575\\n2|-584.408|0.967|1826.938||6.011|3.879\\n3|-576.754|0.965|1828.743|231.384|6.003|3.868\\nstopchunk```\\n\\n\\n\\nOur attempt to use estimated data to complete the set on which we train our models seems to have worked incredibly well. The unseen validation set now has an error of less than 5% if we take the two-variable model.\\n\\nIt does seem surprising that the RMSPE on the unseen validation data is significantly better than the RMSPE on the training data itself, but this can be explained by the fact that we are validating on data near the middle of the year (from the start of April through until the end of June, in this case), and much of the error will come from the end-of-year spike (as suggested by Figure 3). \\n\\n### Third Iteration: Hyper-Personalized Models\\n\\nAt this point we have a fairly good model:\\n\\n$$\\\\hat{S} = \\\\alpha + \\\\beta\\\\zeta_{i} + \\\\gamma\\\\delta_{P} \\\\pm e = S $$ \\n\\nwith $\\\\mathbb{E}(| e |) < \\\\frac{1}{10}S$.\\n\\nWe now improve it further, by leaning further in to the initial insight that we should consider each store individually. We might call this our *personalization thesis*.\\n\\nFor this final iteration of model development, we use `EFS` to find a best two-feature additive linear model for each store independently:\\n\\n$$\\\\Omega_{i} = \\\\alpha_{i} + \\\\beta_{i}f_{i} + \\\\gamma_{i}g_{i} \\\\pm e_{i} = S_{i} $$\\n\\nsuch that we minimize $\\\\mathbb{E}(| e_{i} |)$, with any two features $f_{i}, g_{i} \\\\in \\\\{ \\\\zeta_{i}, \\\\delta_{P}, \\\\delta_{P_{2}}, \\\\delta_{H_{P}}, \\\\delta_{H_{S}}, \\\\delta_{H_{E}}, \\\\delta_{H_{C}} \\\\}$.\\n\\nWe then create a new feature column `IndividualModel2` with the predicted values of that model for each date and store. And then we run `EFS` again, including all the stores together, to evaluate the errors and check whether a better score can be found by including any other feature with `IndividualModel2`.\\n\\n\\n\\n\\nstartchunk```python\\nstart, end = Timestamp(2013,1,1), Timestamp(2015,5,31)\\ntraining_set = integrated.loc[integrated.Date >= start]\\\\\\n                    .loc[integrated.Date <= end]\\nvalidation_set = integrated.loc[integrated.Date >= Timestamp(2015,6,1)]\\\\\\n                    .loc[integrated.Date <= Timestamp(2015,7,31)]\\ndf_store, vdf_store, trend_store = {}, {}, {}\\nbest_predictors, models_store, results_store = {}, {}, {}\\nprocessed_3, trend, mapper = prepare_training_set(start, end, unnecessary)\\nv_set3, v_sales3 = prepare_validation_set(\\n    Timestamp(2015,6,1), Timestamp(2015,7,31),\\n    processed_3, mapper, trend, unnecessary)\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\n## for each store we reduce the training_data and validation_set\\n## ...to that relevant to the store, and then do exhaustive feature selection\\n## ... to suggest the best predictors \\nprocessed_3.drop(columns=\\'Date\\', inplace=True)\\nfor store in range(1,n+1):\\n    df_store[store] = processed_3.loc[training_set.Store == store]\\n    vdf_store[store] = v_set3.loc[validation_set.Store == store]\\n    best_predictors[store] = suggest_best_predictors(df_store[store], 2)\\n    models_store[store], results_store[store] = fit_models(best_predictors[store])\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\npredictions = {}\\nfor store in training_set.Store.unique():\\n    for i in range(1,3):\\n        df_store[store][f\\'IndividualModel{i}\\'] = df_store[store].apply(\\n            lambda x: predict_sales(models_store[store][i], x), axis=1)\\n        vdf_store[store][f\\'IndividualModel{i}\\'] = vdf_store[store].apply(\\n            lambda x: predict_sales(models_store[store][i], x), axis=1)\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\ndf_list, vdf_list = [], []\\nfor store in training_set.Store.unique():\\n    df_list.append(df_store[store])\\n    vdf_list.append(vdf_store[store])\\nind_training = pd.concat(df_list)\\nind_valid = pd.concat(vdf_list).drop(columns=\\'Date\\')\\n## v_set3.drop(columns=\\'Date\\', inplace=True)\\nfor i in range(1,3):\\n    processed_3[f\\'IndividualModel{i}\\'] = ind_training[f\\'IndividualModel{i}\\']\\n    v_set3[f\\'IndividualModel{i}\\'] = ind_valid[f\\'IndividualModel{i}\\']\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\n\\nbest3 = suggest_best_predictors(processed_3, 3)\\nmodels, results3 = fit_models(best3)\\nv_score = {} \\nfor i in models.keys():\\n    v_score[i] = validate_predictions(models[i], v_set3, v_sales3)\\nresults3[\\'Val. RMSPE\\'] = pd.DataFrame(v_score, index=[\\'Score\\']).T\\npandas_df_to_markdown_table(results3)\\nstopchunk```\\n\\nstartchunk```\\nFeatures|Intercept|EasterHoliday|IndividualModel2|Promo|RMSPE|Val.\\nRMSPE\\n---|---|---|---|---|---|---\\n1|0.0||1.0||4.803|2.566\\n2|0.382||0.998|23.864|4.798|2.558\\n3|0.356|-89.409|0.998|23.841|4.797|2.558\\nstopchunk```\\n\\n\\n\\nThis does indeed give improved model performance, and it seems likely our model would perform very well against the test data.\\n\\n## Interpretation and Conclusions\\n\\nMore interesting though than just an improved RMSPE score, is that we can now interrogate the details of each store\\'s best-performing `IndividualModel2` ($\\\\Omega_{i}$) to see what, if anything, the specific details may reveal. Indeed, our method has also yielded an `IndividualModel1`, a best single-feature model for each store. \\n\\nThis means we can see for each store which feature was the most powerful predictor, and which was second. We tabulate the aggregate scores in Table 4 (leaving out the two-feature case from any models which failed to improve the RMSPE score by more than 0.1, meaning the optimum model had already been found).\\n\\n\\n\\nstartchunk```python\\nimprovements = {}\\nfor store in range(1, n+1):\\n    improvements[store] = results_store[store].RMSPE[1]\\\\\\n                    - results_store[store].RMSPE[2]\\n\\nimprovement = pd.DataFrame(improvements, index= [\\'Improvement\\']).T\\nno_improvement = improvement.loc[improvement.Improvement<0.1].index\\n\\npredictor_scores = {}\\nfor i in range(1,3):\\n    predictor_scores[i] = {}\\nfor store in range(1, n+1):\\n    for i in range(1, 3):\\n        for p in best_predictors[store][i]:\\n            if not (store in no_improvement and i == 2):\\n                try:\\n                    predictor_scores[i][p] += 1\\n                except:\\n                    predictor_scores[i][p] = 1\\n\\nwith_one = pd.DataFrame(predictor_scores[1],index=[\\'OnePredictor\\']).T\\nwith_two = pd.DataFrame(predictor_scores[2],index=[\\'TwoPredictors\\']).T\\ncount_predictors = with_two.join(with_one)\\ncount_predictors.fillna(0,inplace=True)\\ncount_predictors[\\'Totals\\'] = count_predictors.OnePredictor \\\\\\n                                + count_predictors.TwoPredictors\\ncount_predictors = count_predictors.astype(int)\\nstopchunk```\\n\\n\\n\\n\\n\\nstartchunk```python\\npandas_df_to_markdown_table(count_predictors.T)\\nstopchunk```\\n\\nstartchunk```\\nindex|Promo|MTD|SchoolHoliday|PublicHoliday|ChristmasHoliday|EasterHoliday\\n---|---|---|---|---|---|---\\nTwoPredictors|1020|1020|10|19|1|2\\nOnePredictor|755|359|0|1|0|0\\nTotals|1775|1379|10|20|1|2\\nstopchunk```\\n\\n\\n\\nTo our surprise, we find that in the `OnePredictor` case, for more than half of the stores the most powerful predictive feature was simply whether or not there was a `Promo` on that day -- rather than the `MTD` $\\\\zeta$ value we have calculated. \\n\\nWe conclude by noting that our models have been simple additive combinations of features -- although those features have included processed multiplicative products. If we needed to try and improve our model still further, we should include the product $\\\\zeta$ and $\\\\delta_{P}$, and perhaps also their products with `StateHoliday`. \\n\\nWe also note that because we treated each store individually, the general effect of features particular to stores is hidden: that is, we see `Promo` and the various types of `StateHoliday` showing up in our final analysis because they are features of a particular `Date`; and we do not see `StoreType` or `Assortment` because when a store is considered individually those features are constant. Also, we have not in the end considered the effect of `Promo2` and `CompetitionDistance`, since their effect is delayed in time and thus complex to detect.\\n\\n## References\\nstartchunk```'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: Forecasting Future Sales\n",
      "---\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The art of predicting the future is both m\n"
     ]
    }
   ],
   "source": [
    "txt.replace('```python','<details>\\n<summary>code</summary>\\n\\n')\n",
    "txt.replace('```python','<details>\\n<summary>code</summary>\\n\\n```python')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
